  % ------------------------------------------------
  % Minimal KDD Explorations Report Template
  % Based on sigkddExp.cls (https://www.kdd.org/author-instructions)
  % ------------------------------------------------

  \documentclass{sigkddExp}

  % --- Title & Author Info ---
  \title{Scalable Local-Recoding Anonymization using Locality
Sensitive Hashing for Big Data Privacy Preservation}

  \numberofauthors{4}

  \author{
    \alignauthor Khotso Bore\\
      \affaddr{University Of Pretoria}\\
      \email{u19180642@tuks.co.za}
    \alignauthor Innocentia Ledimo\\
      \affaddr{University Of Pretoria}\\
      \email{u22928678@tuks.co.za}
    \and
    \alignauthor Busisiwe Vemba\\
      \affaddr{University Of Pretoria}\\
      \email{u22928678@tuks.co.za}
    \alignauthor Siphesihle Khumalo\\
      \affaddr{University Of Pretoria}\\
      \email{u25759257@tuks.co.za}
    }

  \date{\today}

\begin{document}
\maketitle

\begin{abstract}
  A brief summary of your project.
  Mention the dataset, goal (e.g., clustering/classification/EDA), and key findings in 4–5 sentences.
\end{abstract}

% ------------------------------------------------
\section{Exploratory Data Analysis (EDA)}

\subsection{Data Inspection}
Describe your dataset: number of rows, columns, types of variables, missing values, and summary statistics.
Include a short table or paragraph summarizing key properties.

\begin{table}[h]
  \centering
  \caption{Dataset Overview}
  \begin{tabular}{lcc}
    \hline
    Feature & Type        & Missing (\%) \\
    \hline
    Age     & Numerical   & 2.3          \\
    Gender  & Categorical & 0.0          \\
    Income  & Numerical   & 5.1          \\
    \hline
  \end{tabular}
\end{table}

\subsection{Visualisations}

Here are some key visualizations from the EDA phase. We explore the distributions of the numwerical features age and hours worked, and their correlations.

\begin{figure}[h]
  \centering
  \includegraphics[width=0.75\linewidth]{images/age_distribution.png}
  \caption{Age Distribution}
\end{figure}

\begin{figure}[h]
  \centering
  \includegraphics[width=0.75\linewidth]{images/hours_per_week_distribution.png}
  \caption{Hours Per Week Distribution}
\end{figure}

\begin{figure}[h]
  \centering
  \includegraphics[width=0.75\linewidth]{images/age_hours_correlation_matrix.png}
  \caption{Correlation Matrix of Numerical Features}
\end{figure}

\pagebreak
\subsection{Insights}
% Summarize interesting patterns:
% \begin{itemize}
%   \item Which variables correlate strongly?
%   \item Any skewed distributions or outliers?
%   \item Early hypotheses about clusters or classes
% \end{itemize}

The correlation between age and hours worked suggests that while there may be slight tendencies—such as younger or older employees working somewhat more or fewer hours—the relationship is not strong or consistent across the group.

A large portion of employees work standard full-time hours (around 40 hours per week), with fewer employees working significantly more or less than this amount. Very likely because most records are from the United States, with a smaller representation from other countries.

% ------------------------------------------------
\section{Data Preprocessing}

\subsection{Handling Missing Data}

The dataset contained both standard and non-standard forms of missing values, which were handled in two stages:\\

\subsubsection{Initial Removal of Missing Entries:}
After loading the data, all records containing standard missing values were removed. This created a clean baseline for subsequent processing.

\subsubsection{Handling Non-Standard Missing Indicators:}
Some attributes, such as \texttt{workclass}, used the symbol ``?'' to represent missing values instead of a recognized missing data marker, such as \texttt{NaN}. These entries were first identified and converted into proper missing values, after which they were removed from the dataset.

This two-step process ensured that all incomplete records were removed. Maintaining a complete dataset is crucial for the LSH-based anonymization pipeline, since missing values can distort similarity measurements and clustering outcomes.

\subsection{Feature Engineering}
Feature engineering focused on selecting only the attributes relevant for anonymization and preparing them for efficient processing.

\subsubsection{Feature Selection:}
Several columns were removed from the dataset because they were either:
\begin{itemize}
    \item Not quasi-identifiers that could link to external data sources,
    \item Redundant representations of existing information, or
    \item Sensitive or irrelevant for the anonymization process.
\end{itemize}

This reduced the dataset from fifteen to ten columns, retaining only the quasi-identifiers such as \textit{age}, \textit{workclass}, \textit{education}, \textit{marital status}, \textit{occupation}, \textit{relationship}, \textit{race}, \textit{sex}, \textit{native country}, and \textit{hours per week}. These attributes were used as the basis for the local-recoding anonymization.\\

\subsubsection{Data Processing Throughout the LSH-Based \\Anonymization Pipeline:}
Additional transformations were applied throughout the pipeline to ensure that data representation was suitable for the various algorithms at every stage of the pipeline:
\begin{itemize}
    \item Binary vector conversion was used to transform categorical values into formats suitable for MinHash operations.
    \item Distance calculation functions were defined to measure semantic similarity between records.
    \item String standardization was applied to ensure consistent formatting across all categorical values.
\end{itemize}

These preprocessing components established the foundation for the LSH-based anonymization pipeline, which relies on accurate and consistent data representation. Further details on these transformations are discussed in Section~3.

\subsection{Standardisation and Normalisation}
Unlike numerical datasets that require techniques such as z-score standardisation or min--max scaling, this dataset consists mainly of categorical variables. Therefore, normalisation was incorporated directly into the anonymisation pipeline through specialised distance-based approaches:
\begin{itemize}
    \item \textbf{Taxonomy-based distance metrics} that automatically bound all categorical distances within [0, 1] based on tree height and path length.
    \item \textbf{Provenance set-based semantic distances} that ensure comparability across different attributes and taxonomy structures.
    \item \textbf{String trimming} that removes whitespace to ensure consistent categorical value matching.
\end{itemize}

These methods maintain the original categorical form of the data while ensuring that all similarity and distance calculations are standardised. The details of these normalisation techniques are presented in Section~3.


% ------------------------------------------------
\section{Data Mining Methods and Analysis}

\subsection{Methods}
Describe which algorithms or analytical methods were applied:
\begin{itemize}
  \item Clustering (e.g., K-Means, DBSCAN)
  \item Dimensionality Reduction (e.g., PCA, t-SNE)
  \item Classification/Regression (if applicable)
\end{itemize}

\subsection{Results}

\subsection{Information Loss Analysis}

Information loss (ILoss) quantifies the generalization needed to achieve $k$-anonymity, defined as the ratio of unique generalized values to original values across quasi-identifiers. Lower ILoss indicates better data utility.

A parameter sweep across $\alpha \in \{2, 3, 4, 5, 6, 8, 10, 12\}$ illustrates the privacy--utility trade-off in LSH-based anonymization.
\begin{table}[t]
\centering
\footnotesize
\caption{Impact of $\alpha$ on Information Loss and Clustering Quality}
\resizebox{\columnwidth}{!}{%
\begin{tabular}{@{}cccccc@{}}
\toprule
$\alpha$ & ILoss (\%) & Avg Cluster & Total Groups & \% Ideal Range & Time (s) \\ 
\midrule
2  & 50.7 & 197.1 & 153  & 42.5 & 20.9 \\
3  & \textbf{50.4} & 134.1 & 225  & 56.0 & 20.5 \\
4  & 53.4 & 84.3  & 358  & 58.7 & 21.3 \\
6  & 52.5 & 32.5  & 928  & 66.5 & 22.1 \\
8  & 53.3 & 44.8  & 673  & \textbf{74.7} & 23.4 \\
12 & 53.7 & 26.8  & 1127 & 72.2 & 22.0 \\
\bottomrule
\end{tabular}%
}
\end{table}

\noindent \textbf{Findings:}
\begin{itemize}
    \item \textbf{Optimal utility:} $\alpha = 3$ yields the lowest ILoss (50.4\%), aligning with prior recommendations of $\alpha \in [2,4]$.
    \item \textbf{Consistency:} ILoss remains stable (50--54\%) across all $\alpha$, implying taxonomy structure dominates clustering.
    \item \textbf{Trade-off:} Smaller $\alpha$ values form fewer, larger clusters with better utility but weaker $k$-compliance; larger $\alpha$ values improve privacy at minor utility cost.
    \item \textbf{Recommendation:} For $k=10$, $\alpha = 3$ best balances privacy and utility (56\% clusters within ideal range).
\end{itemize}

\noindent High-cardinality attributes (\textit{native-country}, \textit{occupation}) account for about 60\% of total ILoss, suggesting room for optimization via domain-specific taxonomies.



Summarize the main results. Use figures/tables for clarity:
\begin{itemize}
  \item Cluster quality metrics (e.g., silhouette score)
  \item Feature importances
  \item Visualizations of clusters or decision boundaries
\end{itemize}

\subsection{Discussion}
Interpret the results:
\begin{itemize}
  \item What patterns or groups emerged?
  \item Were the methods appropriate?
  \item Any limitations or anomalies?
\end{itemize}

% ------------------------------------------------
\section{Conclusion and Reflection}
Summarize what you found and learned:
\begin{itemize}
  \item Key insights from data mining
  \item Challenges faced and how you overcame them
  \item Potential future work or improvements
\end{itemize}

% ------------------------------------------------
\section*{Acknowledgements}
 (Optional) Acknowledge any data sources, collaborators, or funding.

% ------------------------------------------------
\bibliographystyle{abbrv}
\bibliography{references}

\end{document}
