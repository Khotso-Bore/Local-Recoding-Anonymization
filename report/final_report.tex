% ------------------------------------------------
% Minimal KDD Explorations Report Template
% Based on sigkddExp.cls (https://www.kdd.org/author-instructions)
% ------------------------------------------------

\documentclass{sigkddExp}

% --- Title & Author Info ---
\title{Project Title: A Short, Descriptive Title}

\numberofauthors{4}

\author{
\alignauthor First Author\\
  \affaddr{University / Organization}\\
  \email{first.author@email.com}
\alignauthor Second Author\\
  \affaddr{University / Organization}\\
  \email{second.author@email.com}
}

\date{\today}

\begin{document}
\maketitle

\begin{abstract}
A brief summary of your project.  
Mention the dataset, goal (e.g., clustering/classification/EDA), and key findings in 4–5 sentences.
\end{abstract}

% ------------------------------------------------
\section{Exploratory Data Analysis (EDA)}

\subsection{Data Inspection}
Describe your dataset: number of rows, columns, types of variables, missing values, and summary statistics.  
Include a short table or paragraph summarizing key properties.

\begin{table}[h]
\centering
\caption{Dataset Overview}
\begin{tabular}{lcc}
\hline
Feature & Type & Missing (\%) \\
\hline
Age & Numerical & 2.3 \\
Gender & Categorical & 0.0 \\
Income & Numerical & 5.1 \\
\hline
\end{tabular}
\end{table}

\subsection{Visualisations}
Include key plots to explore distributions or relationships:
\begin{itemize}
  \item Histogram of key numerical features
  \item Boxplot to show outliers
  \item Pairplot / Correlation heatmap for relationships
\end{itemize}

\subsection{Insights}
Summarize interesting patterns:
\begin{itemize}
  \item Which variables correlate strongly?
  \item Any skewed distributions or outliers?
  \item Early hypotheses about clusters or classes
\end{itemize}

% ------------------------------------------------
\section{Data Preprocessing}

\subsection{Handling Missing Data}

The dataset contained both standard and non-standard forms of missing values, which were handled in two stages:\\

\subsubsection{Initial Removal of Missing Entries:}
After loading the data, all records containing standard missing values were removed. This created a clean baseline for subsequent processing.

\subsubsection{Handling Non-Standard Missing Indicators:}
Some attributes, such as \texttt{workclass}, used the symbol ``?'' to represent missing values instead of a recognized missing data marker, such as \texttt{NaN}. These entries were first identified and converted into proper missing values, after which they were removed from the dataset.

This two-step process ensured that all incomplete records were removed. Maintaining a complete dataset is crucial for the LSH-based anonymization pipeline, since missing values can distort similarity measurements and clustering outcomes.

\subsection{Feature Engineering}
Feature engineering focused on selecting only the attributes relevant for anonymization and preparing them for efficient processing.

\subsubsection{Feature Selection:}
Several columns were removed from the dataset because they were either:
\begin{itemize}
    \item Not quasi-identifiers that could link to external data sources,
    \item Redundant representations of existing information, or
    \item Sensitive or irrelevant for the anonymization process.
\end{itemize}

This reduced the dataset from fifteen to ten columns, retaining only the quasi-identifiers such as \textit{age}, \textit{workclass}, \textit{education}, \textit{marital status}, \textit{occupation}, \textit{relationship}, \textit{race}, \textit{sex}, \textit{native country}, and \textit{hours per week}. These attributes were used as the basis for the local-recoding anonymization.\\

\subsubsection{Data Processing Throughout the LSH-Based \\Anonymization Pipeline:}
Additional transformations were applied throughout the pipeline to ensure that data representation was suitable for the various algorithms at every stage of the pipeline:
\begin{itemize}
    \item Binary vector conversion was used to transform categorical values into formats suitable for MinHash operations.
    \item Distance calculation functions were defined to measure semantic similarity between records.
    \item String standardization was applied to ensure consistent formatting across all categorical values.
\end{itemize}

These preprocessing components established the foundation for the LSH-based anonymization pipeline, which relies on accurate and consistent data representation. Further details on these transformations are discussed in Section~3.

\subsection{Standardisation and Normalisation}
Unlike numerical datasets that require techniques such as z-score standardisation or min--max scaling, this dataset consists mainly of categorical variables. Therefore, normalisation was incorporated directly into the anonymisation pipeline through specialised distance-based approaches:
\begin{itemize}
    \item \textbf{Taxonomy-based distance metrics} that automatically bound all categorical distances within [0, 1] based on tree height and path length.
    \item \textbf{Provenance set-based semantic distances} that ensure comparability across different attributes and taxonomy structures.
    \item \textbf{String trimming} that removes whitespace to ensure consistent categorical value matching.
\end{itemize}

These methods maintain the original categorical form of the data while ensuring that all similarity and distance calculations are standardised. The details of these normalisation techniques are presented in Section~3.


% ------------------------------------------------
\section{Data Mining Methods and Analysis}

\subsection{Methods}
The anonymization process treats privacy preservation as a k-member clustering problem under k-anonymity.
 The proposed approach integrates these components:
\begin{itemize}
  \item \textbf{Provenance-Set Semantic Distance:}
Defines similarity between records using Jaccard distance of their provenance sets, enabling LSH to approximate semantic closeness.

  \item \textbf{MinHash-Based LSH Partitioning:} Transforms each record’s provenance vector into MinHash signatures.
Similar records are hashed into the same \(\beta \)-clusters (coarse groups).

  \item \textbf{Recursive k-Member Clustering (LSH-RC):}
Within each \(\beta \)-cluster, a recursive partitioning and agglomerative clustering algorithm is applied.
Combines smaller clusters until each group size \(\geq\) k.

    \item \textbf{Local recording:} Within each cluster, categorical attributes are generalized according to attribute taxonomies, ensuring that each cluster meets the k-anonymity requirement while minimizing information loss. The implementation constructs inverted taxonomies to compute necessary generalizations efficiently.
\end{itemize}

\subsection{Results}
Summarize the main results. Use figures/tables for clarity:
\begin{itemize}
  \item Cluster quality metrics (e.g., silhouette score)
  \item Feature importances
  \item Visualizations of clusters or decision boundaries
\end{itemize}

\subsection{Discussion}
Interpret the results:
\begin{itemize}
  \item What patterns or groups emerged?
  \item Were the methods appropriate?
  \item Any limitations or anomalies?
\end{itemize}

% ------------------------------------------------
\section{Conclusion and Reflection}
Summarize what you found and learned:
\begin{itemize}
  \item Key insights from data mining
  \item Challenges faced and how you overcame them
  \item Potential future work or improvements
\end{itemize}

% ------------------------------------------------
\section*{Acknowledgements}
(Optional) Acknowledge any data sources, collaborators, or funding.

% ------------------------------------------------
\bibliographystyle{abbrv}
\bibliography{references}

\end{document}
