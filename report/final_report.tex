% ------------------------------------------------
% Minimal KDD Explorations Report Template
% Based on sigkddExp.cls (https://www.kdd.org/author-instructions)
% ------------------------------------------------

\documentclass{sigkddExp}

% --- Title & Author Info ---
\title{Project Title: A Short, Descriptive Title}

\numberofauthors{4}

\author{
\alignauthor First Author\\
  \affaddr{University / Organization}\\
  \email{first.author@email.com}
\alignauthor Second Author\\
  \affaddr{University / Organization}\\
  \email{second.author@email.com}
}

\date{\today}

\begin{document}
\maketitle

\begin{abstract}
This report details the implementation and evaluation of a scalable 
local recoding anonymization approach using locality sensitive hashing (LSH), 
primarily based on the research paper "Scalable Local-Recoding Anonymization using 
Locality Sensitive Hashing for Big Data Privacy Preservation" \cite{zhang_scalable_2016}.
This approach proposed to enhance the scalability and efficiency of k-anonymity in big data environments.
Using the UCI Adult dataset \cite{barry_becker_adult_1996} the method integrates a new provenance-set-based
semantic distance metric with LSH(specifically MinHash) for efficient data partition followed by 
a parallelized, recursive agglomerative k-member clustering algorithm(Beta-AC) \cite{zhang_scalable_2016}.
\end{abstract}


% ------------------------------------------------
\section{Exploratory Data Analysis (EDA)}

\subsection{Data Inspection}

The dataset used is a preprocessed version of the Adult dataset from the UCI Machine 
Learning Repository \cite{barry_becker_adult_1996}. It contains over 30000 records across 10 
relevant attributes including 8 quasi-identifiers and 2 numerical features implicitly discretised 
by the methodology. 

The analysis began with loading the Adult dataset consisting of 15 columns initially.

\begin{itemize}
  \item Initial Data Count: The raw dataset had 32561 entries.
  \item column dropping: Several columns were considered non-quasi-identifying or as non-sensitive 
  like capital-gain, capital-loss, fnlwgt, education-num, and income; in order to focus on the key quasi-identifier(QI) 
  attributes. 
  \item Final Quasi identifiers set: The resulting dataset contained 10 columns consisting of 8 categorical 
  attributes(workclass, education, marital-status, occupation, relationship, race, sex, native-country), 
  and two numerical attributes(age, hours-per-week) that would be discretized by the method. Work Class is 
  used as the sensitive attribute. 
  \item Missing Data: The initial inspection revealed missing data represented by a '?' string. These 
  records were removed, reducing the total to 30162 clean records.
\end{itemize}

Describe your dataset: number of rows, columns, types of variables, missing values, and summary statistics.  
Include a short table or paragraph summarizing key properties.

\begin{table}[h]
\centering
\caption{Dataset Overview}
\begin{tabular}{lcc}
\hline
Feature & Type & Missing (\%) \\
\hline
Age & Numerical & 2.3 \\
Gender & Categorical & 0.0 \\
Income & Numerical & 5.1 \\
\hline
\end{tabular}
\end{table}

\subsection{Visualisations}
Include key plots to explore distributions or relationships:
\begin{itemize}
  \item Histogram of key numerical features
  \item Boxplot to show outliers
  \item Pairplot / Correlation heatmap for relationships
\end{itemize}

\subsection{Insights}
Summarize interesting patterns:
\begin{itemize}
  \item Which variables correlate strongly?
  \item Any skewed distributions or outliers?
  \item Early hypotheses about clusters or classes
\end{itemize}

% ------------------------------------------------
\section{Data Preprocessing}



\subsection{Handling Missing Data}
Explain how you dealt with missing data (imputation, deletion, etc.) and justify your choice.

\subsection{Feature Engineering}
List any new variables or transformations you applied (e.g., encoding, log transforms, ratios).

\subsection{Standardisation / Normalisation}
Discuss any scaling applied (e.g., z-score, minâ€“max) and why it was necessary.

% ------------------------------------------------
\section{Data Mining Methods and Analysis}

\subsection{Methods}
Describe which algorithms or analytical methods were applied:
\begin{itemize}
  \item Clustering (e.g., K-Means, DBSCAN)
  \item Dimensionality Reduction (e.g., PCA, t-SNE)
  \item Classification/Regression (if applicable)
\end{itemize}

\subsection{Results}
Summarize the main results. Use figures/tables for clarity:
\begin{itemize}
  \item Cluster quality metrics (e.g., silhouette score)
  \item Feature importances
  \item Visualizations of clusters or decision boundaries
\end{itemize}

\subsection{Discussion}
Interpret the results:
\begin{itemize}
  \item What patterns or groups emerged?
  \item Were the methods appropriate?
  \item Any limitations or anomalies?
\end{itemize}

% ------------------------------------------------
\section{Conclusion and Reflection}
Summarize what you found and learned:
\begin{itemize}
  \item Key insights from data mining
  \item Challenges faced and how you overcame them
  \item Potential future work or improvements
\end{itemize}

% ------------------------------------------------
\section*{Acknowledgements}
(Optional) Acknowledge any data sources, collaborators, or funding.

% ------------------------------------------------
\bibliographystyle{abbrv}
\bibliography{references}

\end{document}
