% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[11pt]{article}

% Change "review" to "final" to generate the final (sometimes called camera-ready) version.
% Change to "preprint" to generate a non-anonymous version with page numbers.
\usepackage[final]{acl}


% Standard package includes
\usepackage{times}
\usepackage{latexsym}

% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

% This is also not strictly necessary, and may be commented out.
% However, it will improve the aesthetics of text in
% the typewriter font.
\usepackage{inconsolata}




%Including images in your LaTeX document requires adding
%additional package(s)
\usepackage{graphicx}

\bibliographystyle{IEEEtran} % Or use "unsrt" if IEEEtran not available


% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{<dim>}
%
% and set <dim> to something 5cm or larger.

\title{SUBWORD-AWARE NEURAL CLASSIFICATION OF SENTIMENT FOR THE SESOTHO LANGUAGE}


% Author block
\author{
  Innocentia Ledimo \quad Khotso Bore \quad Nokubonga Cele \\
  Department of Computer Science, University of Pretoria \\
  COS760 – Natural Language Processing \\
  \texttt{\{u21556564, u19180642, u25717342\}@tuks.co.za}
}


\begin{document}
\maketitle
\begin{abstract}
Neural language models have become central to many NLP tasks like translation and sentiment analysis. To deal with rare or unknown words, these models often break words into smaller parts — a process known as subword tokenization. While this helps in many cases, it can cause problems when applied to African languages like Sesotho and Setswana, where word structure is more complex. In these languages, subword tokenization may lose important meaning, making it harder to understand what’s really being said. Our project explores how this affects sentiment classification. We build a neural model that works with subword tokens and uses STF-IDF to classify text at the word level. We then compare its performance to traditional models like linear regression, as well as pre-trained multilingual models. By doing this, we aim to find out which approaches work best for handling sentiment in low-resource African languages — and how to reduce the loss of meaning along the way.
\end{abstract}

\section{Introduction}

In recent years, neural language models have become the cornerstone of Natural Language Processing (NLP), enabling advances in tasks such as machine translation, sentiment analysis, and information retrieval \cite{mielke2021between}\cite{mesham_low-resource_2021} Central to many of these models is the technique of subword-tokenization, which breaks down words into smaller units to handle rare or unseen terms. However, this technique presents challenges when applied to African languages\cite{buys2021low}. Subword-tokenization can distort meaning and lead to semantic loss, hindering the performance of NLP systems for these languages.

This research addresses the limitations of subword tokenization in neural language models, particularly in the context of sentiment classification for low-resource African languages like Sesotho and Setswana. These languages are underrepresented in both annotated datasets and in the design of existing pre-trained models. 

Our objective is to develop a neural model that utilizes subword tokenization for input but performs classification at the word level. By integrating subword-aware embeddings with traditional techniques such as TF-IDF, and comparing our model's performance with pre-trained multilingual models, we aim to demonstrate more effective sentiment classification for African languages.

Our work directly addresses the issue of under representation in NLP. African languages are vastly under served in mainstream language technologies, leading to a digital divide that marginalizes millions of speakers. By focusing on low-resource languages and developing models that account for their unique linguistic features, we aim to improve fairness and accessibility in NLP.

\section{Background}
Natural Language Processing (NLP) for African languages is gaining momentum, yet remains under-resourced compared to high-resource languages. Recent work has introduced new datasets and modeling techniques, but significant limitations still occur. For instance, the SAfriSenti corpus was developed to support sentiment analysis across several South African languages (e.g. Setswana, Sesotho, Sepedi, isiXhosa, and isiZulu)\cite{SAfriSentiCorpus}, highlighting the effectiveness of multilingual pre-trained language models such as AfroXLMR and AfriBERTa when fine-tuned on related languages \cite{mabokela2024advancing}.
Tokenization strategies have also been evaluated in the context of machine translation. Rajab (2022) compared subword BPE approaches and demonstrated that tokenization quality directly impacts model performance on African languages such as Setswana and isiZulu. SentencePiece tokenization showed improved BLEU scores due to its language-agnostic encoding, which better accommodates the agglutinative structure common in African languages \cite{rajab2022effect}.
Despite these advancements, major gaps remain. Even well-known models like mBERT and XLM-R have limited coverage and training data for African languages\cite{DavlanAfroXLMR}. Moreover, many datasets, such as SAfriSenti, while useful, rely heavily on social media content and may not capture the full linguistic richness of African languages.
Prior research in African NLP has quite a huge difference, but a large gap still exists. Tokenization algorithms, pretrained models, and sentiment datasets often assume linguistic features aligned with European languages. This results in reduced effectiveness for languages such as Setswana and Sesotho. Existing datasets are limited in size and linguistic variation, often made from narrow domains like Twitter, which can introduce bias and restrict generalizability \cite{huang2020multilingual}. 

\section{Methodology}

As previously mentioned, the aim of our study is exploring how effective subword-aware neural language models are, specifically for AfroXLMR\cite{DavlanAfroXLMR} in sentiment analysis with deep focus on African languages that are considered low-resource.With another goal of providing meaningful evaluation, we compared the performance of AfroXLMR with traditional classification models of which include TF-IDF with logistic regression, and a custom subword TF-IDF(STF-IDF) model. 

Our methodology is structured in the following manner: 
Data selection, Preprocessing, Feature engineering, model training, and Evaluation.


To elaborate further on the processes:

\begin{enumerate}
    \item Dataset Collection and Selection
    

    We used the two prescribed datasets as our main sources of data:
    \begin{itemize}
        \item Sesotho subset of the SAfriSenti Corpus. It contained sentiment labeled tweets.\footnote{However, it has recently been removed from Github.}
        \item Sesotho news headlines dataset, locally compiled. This dataset was stored in a custom format, unlabeled consisting of sentences as well as the sentiment integers.
    \end{itemize}

    Choosing the SAfriSenti dataset was influenced by the fact that it was curated  just for sentiment analysis as depicted in its format as well as the contents. It provides content that is user generated in real world. It is also inherently noisy, informal and short, which are characteristics that tend to pose a challenge on standard nlp techniques. However, these are also good characteristics providing an ideal test environment for subword based models with a focus on handling out-of-vocabulary(OOV) terms and spelling variations. The Sesotho news headlines dataset provides a more formal domain complementing the informal tweets dataset, which allows us to evaluate domain robustness of our model. 

    We have selected the Sesotho language because it has proven to be less covered in the mainstream multi lingual sentiment dataset, thus making it an ideal case for cross lingual transfer learning.

    

    \item Preprocessing
    
    
    Preprocessing a dataset is a critical step for both neural and classical approaches to this study. For the traditional aspect, we used a rule-based cleaning approach. This includes the following operations:

    \begin{itemize}
        \item User Mentions: The SAfrisenti tweet had "@user" texts which made sense considering they are tweets but they were unnecessary as per the work and goals we wanted to achieve, thus they had to be removed, as well as any existing URLs as they carry little semantic value and aid with noise. 

        \item Hashtags: punctuations and numbers were eliminated promoting focus on the linguistic content of the sentence.

        \item Lowercasing: For standardization.

        \item Emoji and emoticons: Emojis are not consistently tokenized across models, thus done through stripping unicodes.

        \item Whitespace normalization: This ensures that the content or text is split in a consistent manner, consistent token. This is especially important for tokenizers such as TF-IDF as they are whitespace-sensitive.

        \item Label Standardization: There were also label mismatches between datasets which needed to be corrected before combining them. The SAfrisenti-Corpus dataset had sentiment labels as text. “positive,” “negative,” “neutral” and the Sesotho News headlines had integer labels -1,0,1 for negative, neutral, and positive sentiment respectively. We standardized these to fit as integer classes 0,1,2 for negative, positive, and neutral sentiment respectively

        Many African languages use diacritics and affixes and are very morphologically rich, thus carry semantic meaning. By intentionally retaining these diacritic characters as language specific tokens, we preserve the model's ability to learn meaningful subword patterns and maintain the morphology.
        
    \end{itemize}

    \item Feature Engineering


    Fine Tuning AfroXLMR: Subword-Aware Transformer Model

    Our main model, AfroXLMR was pretrained on 17 African languages. It uses subword tokenization through sentencepiece subword splitting . Thus a good model to get started with.

    We fine-tuned our model for sequence classification task using the Hugging Face Trainer API (WandB):

    \begin{itemize}
        \item The auto-tokenizer split the text or input into subword units and then mapped them to token IDs.
        \item We combined the Sesotho news headlines and the SAfriSenti Sesotho dataset and fine-tuned the model for 3-5 epochs. 
        \item A classification head with softmax output computes probabilities for sentiment classes. 
    \end{itemize}

    AfroXLMR has been optimized specifically for African languages. Its vocab is adapted to low-resource African subwords, which makes it extremely effective on code-switched texts and morphologically rich texts. It has the ability to generalize unseen but linguistically similar languages such as Sesotho which just makes it ideal for our study.

    \item Baseline and Comparative models
    
    TF-IDF and STF-IDF logistic regression models were used as baseline models to set a benchmark for comparison for our neural models.

    We Trained two types of neural models on our combined dataset on classification. We identify these models as the Test models and Competitive models.
    
    For the Test models, the architecture is as follows.

    \begin{itemize}
            \item Input: The input layer would accept subword embeddings or STF-IDF vectors
            \item Hidden layers: 2 hidden layers with dimensions of 128.
            \item Output: An output layer with 3 dimensions for outputs of the 3 classes. Positive, Negative and Neutral
            \item Optimization: Adam optimization
            \item Loss function: CrossEntropyLoss
    \end{itemize}

    The Test models are used as standard indications of potential performance of STF-IDF. From these we developed  models that would accept subword embeddings tokenized using BPE to act as a baseline, comparing the effectiveness of subword tokenization to STF-IDF. The models accepting STF-IDF vectors as inputs used different tokenization methods (N-gram, BPE, Word-Piece)

    Lastly the Competitive models. Somewhat larger, these models were made to compete against the AfroXLMR-Large model. The hope was that larger models would potentially have better performance.

    The model details being: 
    \begin{itemize}
            \item Input: The input layer would accept STF-IDF vectors
            \item Hidden layers: 7 hidden layers with varying dimensions during examination. Hoping more layers would lead to better performance.  
            \item Output: An output layer with 3 dimensions for outputs of the 3 classes. Positive, Negative and Neutral
            \item Optimization: Adam optimization
            \item Loss function: CrossEntropyLoss
    \end{itemize}
\end{enumerate}

\section{Experiments And Results}

 \begin{enumerate}
    \item Evaluation Strategy

    To evaluate our model performance, we used the following metrics.
    \begin{itemize}
    \item Accuracy: This metric depicts the overall correctness of predictions.
    \item Precision: Measures the accuracy of positive predictions. It aims to maximize true positives and minimize false positives.
    \item Recall: Identifies overall positive predictions. It aims to maximize true positives and minimize false negatives
    \item F1-Score: The F1-score is the harmonic mean of both and balances both precision and recall. F1 gives a better sense of real performance especially on imbalanced datasets like ours.
    \end{itemize}

    \item Transformer Performance Evaluation
    
    The AfroXLMR model achieved a validation accuracy of ~60\% on the Sesotho dataset. While it performed well(all things considered) on the dominant negative class, the performance on the minority classes -neutral and positive is lower. The F1-Score indicates the model's moderate aggregate reliability, with precision being the lowest metric. 

    \begin{figure}[h]
    \includegraphics[width=7cm]{latex/evalution_images/confusion_matrix.png}
    \centering
    \caption{AfroXLMR Confusion Matrix}
    \end{figure}

    Adding on to our evaluations, the model's confusion matrix reveals that the model does demonstrate a strong performance in identifying positive sentiments, correctly classifying 528 examples. However, it under performs significantly on negative and neutral classes, with many negative samples incorrectly labeled as positive. Similarly, the neutral class is also incorrectly classified, likely due to its under-representation in the dataset. This imbalance highlights a bias toward the positive class, which skews the decision boundaries of the model.

    
    \item Test Model Analysis
    
    Results from the Test models show weighted-average F1-scores of 59\% and greater for the STF-IDF models. The best being the STF-IDF Word-Piece model. These scores are considerably higher than the test model which uses standard embeddings tokenization with BPE. This model scored a weighted-average F1-score of 54\%

    \begin{figure}[h]
    \includegraphics[width=7cm]{evalution_images/neural_model_comparissons.png}
    \centering
    \end{figure}
    
   
    \item Competitive model Hyperparameter tuning

    For the Competitive models we extensively tuned the hyper-parameters like training epochs, learning rate and hidden layer dimensions to find the best possible model for each subword tokenization method (N-gram, BPE, Word-Piece) combined with TF-IDF

    \begin{figure}[h]
    \includegraphics[width=7cm]{evalution_images/ngram.png}
    \centering
    \caption{The best N-gram model configuration had dimensions:256, Epochs:20 and Learning rate: 0.001}
    \end{figure}

    \begin{figure}[h]
    \includegraphics[width=7cm]{evalution_images/bpe.png}
    \centering
    \caption{The best BPE model configuration had dimensions:128, Epochs:10 and Learning rate: 0.001}
    \end{figure}

    \begin{figure}[h]
    \includegraphics[width=7cm]{evalution_images/word_piece.png}
    \centering
    \caption{The best Word-piece model configuration had dimensions:256, Epochs:15 and Learning rate: 0.001}
    \end{figure}
    
    \newpage
    \item Comparative model analysis

    From our tuned Competitive models on 3 different subword tokenization methods (N-gram, BPE and Word-Piece) combined with TF-IDF. The STF-IDF neural models scored F1-scores above 59\%, the best model being the STF-IDF BPE model scoring 62.55\%. All considerably higher than any of the comparative models which scored 46.67\%, 45.14\% and 46.77\% for the TF-IDF + Logistic regression, BPE STF-IDF + Logistic regression and AfroXLMR-large model respectively

    \begin{figure}[h]
    \includegraphics[width=7cm]{evalution_images/final_eval.png}
    \centering
    \caption{Model comparisons}
    \end{figure}
\end{enumerate}


\section{Reflections and Discussion}
What worked.

Model selection: The choice of AfroXLMR was appropriate for multilingual tasks and low-resource languages, ensuring coverage of African languages that are often underrepresented in mainstream NLP models.

Data sourcing: The effort to increase the dataset size from 3000 to 5000 improved model performance potential, showing adaptability and resourcefulness.

Platform utilization: Google Colab provided accessible GPU resources for model training without additional cost.

What Didn’t Work.

Data quantity constraints: The limited number of samples was still small for transformer models, which typically require significantly larger datasets to generalize well. This affected the model’s overall accuracy and robustness.

Computational Limits: Training on Google Colab faced repeated interruptions, session limits, and runtime errors. These factors restricted experimentation, hyperparameter tuning, and thorough model validation.

Lessons Learned: Need for scalable infrastructure: For large-scale models like AfroXLMR, more stable and scalable compute resources 

Importance of data quality and quantity: Sufficient and clean data significantly impacts model performance. Future projects should allocate more time for data collection, cleaning, and augmentation.

Experimentation limitations: Resource-constrained environments limit the ability to perform necessary iterations, affecting the ability to explore different model architectures or training strategies.
Possible Extensions or Improvements

\section{Conclusion}

This research contributes both the methodological and practical insights and solutions to the African languages in NLP. Our research findings and experiments demonstrate that innovative hybrid approaches can effectively bridge the gap between traditional methods and modern neural architectures, providing a promising path for African language technology development across the African continent. The depicted success of our STF-IDF neural models, which achieved F1-scores of 62.55\% compared to 46.77\% for AfroXLMR large, challenges the notion that larger pre-trained models are always superior for low-resource languages. By developing effective NLP tools for Sesotho, this work takes an effective step toward reducing the digital divide that has marginalized millions of African language speakers. While challenges such as data scarcity and computational constraints remain, our research establishes that resource-efficient hybrid approaches may be strong candidates and more viable than purely transformer-based solutions for organizations working with limited resources. As we move forward, the success of our approach promotes NLP investigation across other African languages and highlights the need for collaborative approaches to develop larger, more varied datasets for continuous advancement in African NLP research.


% \subsection{Footnotes}

% Footnotes are inserted with the \verb|\footnote| command.\footnote{This is a footnote.}

% \subsection{Tables and figures}

% See Table~\ref{tab:accents} for an example of a table and its caption.
% \textbf{Do not override the default caption sizes.}

% \begin{table}
%   \centering
%   \begin{tabular}{lc}
%     \hline
%     \textbf{Command} & \textbf{Output} \\
%     \hline
%     \verb|{\"a}|     & {\"a}           \\
%     \verb|{\^e}|     & {\^e}           \\
%     \verb|{\`i}|     & {\`i}           \\
%     \verb|{\.I}|     & {\.I}           \\
%     \verb|{\o}|      & {\o}            \\
%     \verb|{\'u}|     & {\'u}           \\
%     \verb|{\aa}|     & {\aa}           \\\hline
%   \end{tabular}
%   \begin{tabular}{lc}
%     \hline
%     \textbf{Command} & \textbf{Output} \\
%     \hline
%     \verb|{\c c}|    & {\c c}          \\
%     \verb|{\u g}|    & {\u g}          \\
%     \verb|{\l}|      & {\l}            \\
%     \verb|{\~n}|     & {\~n}           \\
%     \verb|{\H o}|    & {\H o}          \\
%     \verb|{\v r}|    & {\v r}          \\
%     \verb|{\ss}|     & {\ss}           \\
%     \hline
%   \end{tabular}
%   \caption{Example commands for accented characters, to be used in, \emph{e.g.}, Bib\TeX{} entries.}
%   \label{tab:accents}
% \end{table}

% As much as possible, fonts in figures should conform
% to the document fonts. See Figure~\ref{fig:experiments} for an example of a figure and its caption.

% Using the \verb|graphicx| package graphics files can be included within figure
% environment at an appropriate point within the text.
% The \verb|graphicx| package supports various optional arguments to control the
% appearance of the figure.
% You must include it explicitly in the \LaTeX{} preamble (after the
% \verb|\documentclass| declaration and before \verb|\begin{document}|) using
% \verb|\usepackage{graphicx}|.

% \begin{figure}[t]
%   \includegraphics[width=\columnwidth]{example-image-golden}
%   \caption{A figure with a caption that runs for more than one line.
%     Example image is usually available through the \texttt{mwe} package
%     without even mentioning it in the preamble.}
%   \label{fig:experiments}
% \end{figure}

% \begin{figure*}[t]
%   \includegraphics[width=0.48\linewidth]{example-image-a} \hfill
%   \includegraphics[width=0.48\linewidth]{example-image-b}
%   \caption {A minimal working example to demonstrate how to place
%     two images side-by-side.}
% \end{figure*}

% \subsection{Hyperlinks}

% Users of older versions of \LaTeX{} may encounter the following error during compilation:
% \begin{quote}
% \verb|\pdfendlink| ended up in different nesting level than \verb|\pdfstartlink|.
% \end{quote}
% This happens when pdf\LaTeX{} is used and a citation splits across a page boundary. The best way to fix this is to upgrade \LaTeX{} to 2018-12-01 or later.

% \subsection{Citations}

% \begin{table*}
%   \centering
%   \begin{tabular}{lll}
%     \hline
%     \textbf{Output}           & \textbf{natbib command} & \textbf{ACL only command} \\
%     \hline
%     \citep{Gusfield:97}       & \verb|\citep|           &                           \\
%     \citealp{Gusfield:97}     & \verb|\citealp|         &                           \\
%     \citet{Gusfield:97}       & \verb|\citet|           &                           \\
%     \citeyearpar{Gusfield:97} & \verb|\citeyearpar|     &                           \\
%     \citeposs{Gusfield:97}    &                         & \verb|\citeposs|          \\
%     \hline
%   \end{tabular}
%   \caption{\label{citation-guide}
%     Citation commands supported by the style file.
%     The style is based on the natbib package and supports all natbib citation commands.
%     It also supports commands defined in previous ACL style files for compatibility.
%   }
% \end{table*}

% Table~\ref{citation-guide} shows the syntax supported by the style files.
% We encourage you to use the natbib styles.
% You can use the command \verb|\citet| (cite in text) to get ``author (year)'' citations, like this citation to a paper by \citet{Gusfield:97}.
% You can use the command \verb|\citep| (cite in parentheses) to get ``(author, year)'' citations \citep{Gusfield:97}.
% You can use the command \verb|\citealp| (alternative cite without parentheses) to get ``author, year'' citations, which is useful for using citations within parentheses (e.g. \citealp{Gusfield:97}).

% A possessive citation can be made with the command \verb|\citeposs|.
% This is not a standard natbib command, so it is generally not compatible
% with other style files.

% \subsection{References}

% \nocite{Ando2005,andrew2007scalable,rasooli-tetrault-2015}

% The \LaTeX{} and Bib\TeX{} style files provided roughly follow the American Psychological Association format.
% If your own bib file is named \texttt{custom.bib}, then placing the following before any appendices in your \LaTeX{} file will generate the references section for you:
% \begin{quote}
% \begin{verbatim}
% \bibliography{custom}
% \end{verbatim}
% \end{quote}

% You can obtain the complete ACL Anthology as a Bib\TeX{} file from \url{https://aclweb.org/anthology/anthology.bib.gz}.
% To include both the Anthology and your own .bib file, use the following instead of the above.
% \begin{quote}
% \begin{verbatim}
% \bibliography{anthology,custom}
% \end{verbatim}
% \end{quote}

% Please see Section~\ref{sec:bibtex} for information on preparing Bib\TeX{} files.

% \subsection{Equations}

% An example equation is shown below:
% \begin{equation}
%   \label{eq:example}
%   A = \pi r^2
% \end{equation}

% Labels for equation numbers, sections, subsections, figures and tables
% are all defined with the \verb|\label{label}| command and cross references
% to them are made with the \verb|\ref{label}| command.

% This an example cross-reference to Equation~\ref{eq:example}.

% \subsection{Appendices}

% Use \verb|\appendix| before any appendix section to switch the section numbering over to letters. See Appendix~\ref{sec:appendix} for an example.

% \section{Bib\TeX{} Files}
% \label{sec:bibtex}

% Unicode cannot be used in Bib\TeX{} entries, and some ways of typing special characters can disrupt Bib\TeX's alphabetization. The recommended way of typing special characters is shown in Table~\ref{tab:accents}.

% Please ensure that Bib\TeX{} records contain DOIs or URLs when possible, and for all the ACL materials that you reference.
% Use the \verb|doi| field for DOIs and the \verb|url| field for URLs.
% If a Bib\TeX{} entry has a URL or DOI field, the paper title in the references section will appear as a hyperlink to the paper, using the hyperref \LaTeX{} package.

% \section*{Limitations}

% Since December 2023, a "Limitations" section has been required for all papers submitted to ACL Rolling Review (ARR). This section should be placed at the end of the paper, before the references. The "Limitations" section (along with, optionally, a section for ethical considerations) may be up to one page and will not count toward the final page limit. Note that these files may be used by venues that do not rely on ARR so it is recommended to verify the requirement of a "Limitations" section and other criteria with the venue in question.

% \section*{Acknowledgments}

% This document has been adapted
% by Steven Bethard, Ryan Cotterell and Rui Yan
% from the instructions for earlier ACL and NAACL proceedings, including those for
% ACL 2019 by Douwe Kiela and Ivan Vuli\'{c},
% NAACL 2019 by Stephanie Lukin and Alla Roskovskaya,
% ACL 2018 by Shay Cohen, Kevin Gimpel, and Wei Lu,
% NAACL 2018 by Margaret Mitchell and Stephanie Lukin,
% Bib\TeX{} suggestions for (NA)ACL 2017/2018 from Jason Eisner,
% ACL 2017 by Dan Gildea and Min-Yen Kan,
% NAACL 2017 by Margaret Mitchell,
% ACL 2012 by Maggie Li and Michael White,
% ACL 2010 by Jing-Shin Chang and Philipp Koehn,
% ACL 2008 by Johanna D. Moore, Simone Teufel, James Allan, and Sadaoki Furui,
% ACL 2005 by Hwee Tou Ng and Kemal Oflazer,
% ACL 2002 by Eugene Charniak and Dekang Lin,
% and earlier ACL and EACL formats written by several people, including
% John Chen, Henry S. Thompson and Donald Walker.
% Additional elements were taken from the formatting instructions of the \emph{International Joint Conference on Artificial Intelligence} and the \emph{Conference on Computer Vision and Pattern Recognition}.

% % Bibliography entries for the entire Anthology, followed by custom entries
% %\bibliography{anthology,custom}
% % Custom bibliography entries only
% \bibliography{custom}

% \appendix

% \section{Example Appendix}
% \label{sec:appendix}

% This is an appendix.
\bibliography{NLP-PROJECT-ref}

\end{document}
