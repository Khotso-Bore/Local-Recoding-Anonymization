@article{mielke2021between,
  author    = {S. J. Mielke and Z. Alyafeai and E. Salesky and others},
  title     = {Between words and characters: A brief history of open-vocabulary modeling and tokenization in NLP},
  journal   = {arXiv preprint arXiv:2112.10508},
  year      = {2021},
  url       = {https://arxiv.org/pdf/2112.10508},
  eprint    = {2112.10508},
  archivePrefix = {arXiv},
  primaryClass  = {cs.CL}
}

@article{buys2021low,
  author    = {S. M. L. H. J. S. J. Buys},
  title     = {Low-Resource Language Modelling of South African Language},
  journal   = {arXiv preprint arXiv:2104.00772},
  volume    = {1},
  pages     = {10},
  year      = {2021},
  eprint    = {2104.00772},
  archivePrefix = {arXiv},
  primaryClass  = {cs.CL}
}

@inproceedings{rajab2022effect,
  author    = {J. Rajab},
  title     = {Effect of Tokenisation Strategies for Low-Resourced Southern African Languages},
  booktitle = {AfricaNLP Workshop at ICLR 2022},
  pages     = {8},
  year      = {2022}
}

@article{mabokela2024advancing,
  author    = {Koena Ronny Mabokela and M. P. T. C.},
  title     = {Advancing sentiment analysis for low-resourced African languages using pre-trained language models},
  journal   = {PLOS ONE},
  pages     = {37},
  year      = {2024}
}

@article{huang2020multilingual,
  author       = {Xiaolei Huang and Linzi Xing and Franck Dernoncourt and Michael J. Paul},
  title        = {Multilingual Twitter Corpus and Baselines for Evaluating Demographic Bias in Hate Speech Recognition},
  journal      = {arXiv preprint arXiv:2002.10361},
  year         = {2020},
  month        = mar,
  eprint       = {2002.10361},
  archivePrefix= {arXiv},
  primaryClass = {cs.CL},
  url          = {https://arxiv.org/abs/2002.10361}
}
@article{ugwu_part_2024,
	title = {A part of speech tagger for {Yoruba} language text using deep neural network},
	volume = {9},
	issn = {2773-1863},
	url = {https://www.sciencedirect.com/science/article/pii/S2773186324001154},
	doi = {10.1016/j.fraope.2024.100185},
	abstract = {The pursuit of advancing Yoruba language in the realm of technology has underscored the necessity for an efficient foundational natural language processing (NLP) tool, notably the part-of-speech (POS) tagger. POS tagging serves as the building block to numerous NLP applications, as its capacity to recognize and assign appropriate syntactic tags to words is pivotal to the efficiency of NLP solutions. However, the existing POS taggers for Yoruba language either rely on rule-based approaches, which are limited by the comprehensiveness and accuracy of the defined rules; or stochastic approaches, which are extremely redundant in generating sequence of tags. Hence, this paper advocates the utilization of machine learning models to develop robust and highly effective POS taggers tailored to Yoruba text. Specifically, a Feed Forward Deep Neural Network (FF-DNN) was employed and trained using curated Yoruba tag set sourced from Yoruba religion and dictionary texts, comprising 20,795 words alongside their corresponding POS tags. The evaluation of the model demonstrates an accuracy of 99 \% and a precision of 98 \% in predicting appropriate tags, outperforming Random Forest (RF), Logistic Regression (LR), and K-Nearest Neighbour (k-NN) machine learning models.},
	urldate = {2025-04-12},
	journal = {Franklin Open},
	author = {Ugwu, Chukwuemeka Christian and Oyewole, Abisola Rukayat and Popoola, Olugbemiga Solomon and Adetunmbi, Adebayo Olusola and Elebute, Ayo},
	month = dec,
	year = {2024},
	keywords = {Natural language processing, Neural network, Part-of-speech, Tagger, Yoruba language},
	pages = {100185},
	file = {ScienceDirect Snapshot:files/157/S2773186324001154.html:text/html},
}

@misc{mesham_low-resource_2021,
	title = {Low-{Resource} {Language} {Modelling} of {South} {African} {Languages}},
	url = {http://arxiv.org/abs/2104.00772},
	doi = {10.48550/arXiv.2104.00772},
	abstract = {Language models are the foundation of current neural network-based models for natural language understanding and generation. However, research on the intrinsic performance of language models on African languages has been extremely limited, which is made more challenging by the lack of large or standardised training and evaluation sets that exist for English and other high-resource languages. In this paper, we evaluate the performance of open-vocabulary language models on low-resource South African languages, using byte-pair encoding to handle the rich morphology of these languages. We evaluate different variants of n-gram models, feedforward neural networks, recurrent neural networks (RNNs), and Transformers on small-scale datasets. Overall, well-regularized RNNs give the best performance across two isiZulu and one Sepedi datasets. Multilingual training further improves performance on these datasets. We hope that this research will open new avenues for research into multilingual and low-resource language modelling for African languages.},
	urldate = {2025-04-12},
	publisher = {arXiv},
	author = {Mesham, Stuart and Hayward, Luc and Shapiro, Jared and Buys, Jan},
	month = apr,
	year = {2021},
	note = {arXiv:2104.00772 [cs]},
	keywords = {Computer Science - Computation and Language},
	annote = {Comment: AfricaNLP workshop at EACL 2021},
	file = {Full Text PDF:files/161/Mesham et al. - 2021 - Low-Resource Language Modelling of South African Languages.pdf:application/pdf;Snapshot:files/160/2104.html:text/html},
}

@article{shikali_enhancing_2020,
	title = {Enhancing {African} low-resource languages: {Swahili} data for language modelling},
	volume = {31},
	issn = {23523409},
	shorttitle = {Enhancing {African} low-resource languages},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S2352340920308453},
	doi = {10.1016/j.dib.2020.105951},
	abstract = {Language modelling using neural networks requires adequate data to guarantee quality word representation which is important for natural language processing (NLP) tasks. However, African languages, Swahili in particular, have been disadvantaged and most of them are classiﬁed as low resource languages because of inadequate data for NLP. In this article, we derive and contribute unannotated Swahili dataset, Swahili syllabic alphabet and Swahili word analogy dataset to address the need for language processing resources especially for low resource languages. Therefore, we derive the unannotated Swahili dataset by pre-processing raw Swahili data using a Python script, formulate the syllabic alphabet and develop the Swahili word analogy dataset based on an existing English dataset. We envisage that the datasets will not only support language models but also other NLP downstream tasks such as part-of-speech tagging, machine translation and sentiment analysis.},
	language = {en},
	urldate = {2025-04-12},
	journal = {Data in Brief},
	author = {Shikali, Casper S. and Mokhosi, Refuoe},
	month = aug,
	year = {2020},
	pages = {105951},
	file = {PDF:files/162/Shikali and Mokhosi - 2020 - Enhancing African low-resource languages Swahili data for language modelling.pdf:application/pdf},
}

@article{mielke2021between,
  author    = {S. J. Mielke and Z. Alyafeai and E. Salesky and others},
  title     = {Between words and characters: A brief history of open-vocabulary modeling and tokenization in NLP},
  journal   = {arXiv preprint arXiv:2112.10508},
  year      = {2021},
  url       = {https://arxiv.org/pdf/2112.10508},
  eprint    = {2112.10508},
  archivePrefix = {arXiv},
  primaryClass  = {cs.CL}
}

@article{buys2021low,
  author    = {S. M. L. H. J. S. J. Buys},
  title     = {Low-Resource Language Modelling of South African Language},
  journal   = {arXiv preprint arXiv:2104.00772},
  volume    = {1},
  pages     = {10},
  year      = {2021},
  eprint    = {2104.00772},
  archivePrefix = {arXiv},
  primaryClass  = {cs.CL}
}

@inproceedings{rajab2022effect,
  author    = {J. Rajab},
  title     = {Effect of Tokenisation Strategies for Low-Resourced Southern African Languages},
  booktitle = {AfricaNLP Workshop at ICLR 2022},
  pages     = {8},
  year      = {2022}
}

@article{mabokela2024advancing,
  author    = {Koena Ronny Mabokela and M. P. T. C.},
  title     = {Advancing sentiment analysis for low-resourced African languages using pre-trained language models},
  journal   = {PLOS ONE},
  pages     = {37},
  year      = {2024}
}

@article{huang2020multilingual,
  author       = {Xiaolei Huang and Linzi Xing and Franck Dernoncourt and Michael J. Paul},
  title        = {Multilingual Twitter Corpus and Baselines for Evaluating Demographic Bias in Hate Speech Recognition},
  journal      = {arXiv preprint arXiv:2002.10361},
  year         = {2020},
  month        = mar,
  eprint       = {2002.10361},
  archivePrefix= {arXiv},
  primaryClass = {cs.CL},
  url          = {https://arxiv.org/abs/2002.10361}
}


@article{ugwu_part_2024,
	title = {A part of speech tagger for {Yoruba} language text using deep neural network},
	volume = {9},
	issn = {2773-1863},
	url = {https://www.sciencedirect.com/science/article/pii/S2773186324001154},
	doi = {10.1016/j.fraope.2024.100185},
	abstract = {The pursuit of advancing Yoruba language in the realm of technology has underscored the necessity for an efficient foundational natural language processing (NLP) tool, notably the part-of-speech (POS) tagger. POS tagging serves as the building block to numerous NLP applications, as its capacity to recognize and assign appropriate syntactic tags to words is pivotal to the efficiency of NLP solutions. However, the existing POS taggers for Yoruba language either rely on rule-based approaches, which are limited by the comprehensiveness and accuracy of the defined rules; or stochastic approaches, which are extremely redundant in generating sequence of tags. Hence, this paper advocates the utilization of machine learning models to develop robust and highly effective POS taggers tailored to Yoruba text. Specifically, a Feed Forward Deep Neural Network (FF-DNN) was employed and trained using curated Yoruba tag set sourced from Yoruba religion and dictionary texts, comprising 20,795 words alongside their corresponding POS tags. The evaluation of the model demonstrates an accuracy of 99 \% and a precision of 98 \% in predicting appropriate tags, outperforming Random Forest (RF), Logistic Regression (LR), and K-Nearest Neighbour (k-NN) machine learning models.},
	urldate = {2025-04-12},
	journal = {Franklin Open},
	author = {Ugwu, Chukwuemeka Christian and Oyewole, Abisola Rukayat and Popoola, Olugbemiga Solomon and Adetunmbi, Adebayo Olusola and Elebute, Ayo},
	month = dec,
	year = {2024},
	keywords = {Natural language processing, Neural network, Part-of-speech, Tagger, Yoruba language},
	pages = {100185},
	file = {ScienceDirect Snapshot:files/157/S2773186324001154.html:text/html},
}

@online{dey2023understanding,
  author = {Roshmitta Dey},
  title = {Understanding Language Modeling: From N-grams to Transformer-based Neural Models},
  year = {2023},
  url = {https://medium.com/@roshmitadey/understanding-language-modeling-from-n-grams-to-transformer-based-neural-models-d2bdf1532c6d},
 
}

@article{mielke2021between,
  author = {Sabrina J. Mielke and Zaid Alyafeai and Elizabeth Salesky and Colin Raffel and Manan Dey and Matthias Gallé and Arun Raja and Chenglei Si and Wilson Y. Lee and Benoît Sagot and Samson Tan},
  title = {Between Words and Characters: A Brief History of Open-Vocabulary Modeling and Tokenization in NLP},
  year = {2021},
  eprint = {2112.10508},
  archivePrefix = {arXiv},
  primaryClass = {cs.CL},
  url = {https://arxiv.org/pdf/2112.10508},
  }


@misc{SAfriSentiCorpus,
  author       = {Ronny Mabokela},
  title        = {SAfriSenti-Corpus: A multilingual sentiment corpus for South African Under-Resourced languages},
  year         = {2025},
  publisher    = {GitHub},
  note         = {Repository for sentiment analysis in South African languages},
  url          = {https://github.com/NLPforLRLsProjects/SAfriSenti-Corpus}
}

@misc{DavlanAfroXLMR,
  author       = {Davlan},
  title        = {AfroXLMR-large: A Multilingual Language Model for African Languages},
  year         = {2022},
  publisher    = {Hugging Face},
  note         = {Repository for AfroXLMR-large, adapted for African languages},
  url          = {https://huggingface.co/Davlan/afro-xlmr-large},
}


@article{salunkeenhancing,
  title={Enhancing Contextual Understanding in NLP: A Subword Tokenization Approach with ELMo and BERT},
  author={Salunke, Aatmaj Amol}
}

@misc{mesham_low-resource_2021,
	title = {Low-{Resource} {Language} {Modelling} of {South} {African} {Languages}},
	url = {http://arxiv.org/abs/2104.00772},
	doi = {10.48550/arXiv.2104.00772},
	abstract = {Language models are the foundation of current neural network-based models for natural language understanding and generation. However, research on the intrinsic performance of language models on African languages has been extremely limited, which is made more challenging by the lack of large or standardised training and evaluation sets that exist for English and other high-resource languages. In this paper, we evaluate the performance of open-vocabulary language models on low-resource South African languages, using byte-pair encoding to handle the rich morphology of these languages. We evaluate different variants of n-gram models, feedforward neural networks, recurrent neural networks (RNNs), and Transformers on small-scale datasets. Overall, well-regularized RNNs give the best performance across two isiZulu and one Sepedi datasets. Multilingual training further improves performance on these datasets. We hope that this research will open new avenues for research into multilingual and low-resource language modelling for African languages.},
	urldate = {2025-04-12},
	publisher = {arXiv},
	author = {Mesham, Stuart and Hayward, Luc and Shapiro, Jared and Buys, Jan},
	month = apr,
	year = {2021},
	note = {arXiv:2104.00772 [cs]},
	keywords = {Computer Science - Computation and Language},
	annote = {Comment: AfricaNLP workshop at EACL 2021},
	file = {Full Text PDF:files/161/Mesham et al. - 2021 - Low-Resource Language Modelling of South African Languages.pdf:application/pdf;Snapshot:files/160/2104.html:text/html},
}

@article{shikali_enhancing_2020,
	title = {Enhancing {African} low-resource languages: {Swahili} data for language modelling},
	volume = {31},
	issn = {23523409},
	shorttitle = {Enhancing {African} low-resource languages},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S2352340920308453},
	doi = {10.1016/j.dib.2020.105951},
	abstract = {Language modelling using neural networks requires adequate data to guarantee quality word representation which is important for natural language processing (NLP) tasks. However, African languages, Swahili in particular, have been disadvantaged and most of them are classiﬁed as low resource languages because of inadequate data for NLP. In this article, we derive and contribute unannotated Swahili dataset, Swahili syllabic alphabet and Swahili word analogy dataset to address the need for language processing resources especially for low resource languages. Therefore, we derive the unannotated Swahili dataset by pre-processing raw Swahili data using a Python script, formulate the syllabic alphabet and develop the Swahili word analogy dataset based on an existing English dataset. We envisage that the datasets will not only support language models but also other NLP downstream tasks such as part-of-speech tagging, machine translation and sentiment analysis.},
	language = {en},
	urldate = {2025-04-12},
	journal = {Data in Brief},
	author = {Shikali, Casper S. and Mokhosi, Refuoe},
	month = aug,
	year = {2020},
	pages = {105951},
	file = {PDF:files/162/Shikali and Mokhosi - 2020 - Enhancing African low-resource languages Swahili data for language modelling.pdf:application/pdf},
}