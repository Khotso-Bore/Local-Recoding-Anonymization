{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5a7752ec",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/Khotso-Bore/Local-Recoding-Anonymization/blob/Innocentia's/local_recoding.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "id": "9eeb591d",
   "metadata": {
    "id": "9eeb591d"
   },
   "source": [
    "from pprint import pprint\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "36d073d1",
   "metadata": {
    "id": "36d073d1"
   },
   "source": [
    "### Prepare Dataset"
   ]
  },
  {
   "cell_type": "code",
   "id": "b6c3f0ff",
   "metadata": {
    "id": "b6c3f0ff"
   },
   "source": [
    "df = pd.read_csv('adult/adult.data', header=None,names=[\n",
    "    \"age\",\n",
    "    \"workclass\",\n",
    "    \"fnlwgt\",\n",
    "    \"education\",\n",
    "    \"education-num\",\n",
    "    \"marital-status\",\n",
    "    \"occupation\",\n",
    "    \"relationship\",\n",
    "    \"race\",\n",
    "    \"sex\",\n",
    "    \"capital-gain\",\n",
    "    \"capital-loss\",\n",
    "    \"hours-per-week\",\n",
    "    \"native-country\",\n",
    "    \"income\"\n",
    "]\n",
    " )"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "c2e7044a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 400
    },
    "id": "c2e7044a",
    "outputId": "21d9e040-5ae5-4b24-8894-3c74128d18d9"
   },
   "source": [
    "df.head()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "536839c0",
   "metadata": {
    "id": "536839c0"
   },
   "source": [
    "df.dropna(inplace=True)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "08f3d453",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "08f3d453",
    "outputId": "b9462e8b-6b7a-4316-e6f4-14effc4b75c2"
   },
   "source": [
    "df.info()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "9184ae00",
   "metadata": {
    "id": "9184ae00"
   },
   "source": [
    "### drop coloumns"
   ]
  },
  {
   "cell_type": "code",
   "id": "3132e830",
   "metadata": {
    "id": "3132e830"
   },
   "source": [
    "drop_columns = ['capital-gain', 'capital-loss', 'fnlwgt', 'education-num','income']\n",
    "\n",
    "'''\n",
    "# Drop unnecessary columns\n",
    "These columns are dropped as they are not needed as\n",
    "they may not contain any sensitive information.\n",
    "required for the local recoding anonymization process.\n",
    "'''\n",
    "df.drop(columns=drop_columns, inplace=True)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "d9c81b94",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d9c81b94",
    "outputId": "525b3777-4eff-4525-9ba2-5a5bdd7ac0e3"
   },
   "source": [
    "# count number of columns\n",
    "print(f\"Number of columns after dropping unnecessary columns: {len(df.columns)}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "a699145c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a699145c",
    "outputId": "187b8b57-7fc2-4848-eaca-a1191b3e0357"
   },
   "source": [
    "df.info()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "65160a8d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 415
    },
    "id": "65160a8d",
    "outputId": "3a6bd24a-4cc8-4632-9449-8df62a3741ec"
   },
   "source": [
    "print(\"Sensitive Attribute - workclass value counts:\")\n",
    "df['workclass'].value_counts()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "c6976471",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c6976471",
    "outputId": "03913490-80ce-493f-afdb-3a0b7aa41d4c"
   },
   "source": [
    "# Show the first row where 'workclass' contains '?'\n",
    "missing_value = df[df['workclass'].str.contains('?', regex=False)]['workclass'].head(1).values[0]\n",
    "print(f\"Missing value representation in 'workclass': {missing_value}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "51102134",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 335
    },
    "id": "51102134",
    "outputId": "99bf88f8-c422-4353-be79-94c9b7fb4598"
   },
   "source": [
    "df.replace(missing_value,np.nan,inplace=True)\n",
    "df.dropna(inplace=True)\n",
    "df['workclass'].value_counts()\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "dd08af7a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 335
    },
    "id": "dd08af7a",
    "outputId": "b83bd7cb-6021-4604-99fe-0a88a52c82a3"
   },
   "source": [
    "df['marital-status'].value_counts()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "aeaec8ea",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "aeaec8ea",
    "outputId": "3ef9d070-6e69-4ce8-bfa8-4de56b24871b"
   },
   "source": [
    "#plot the distribution of every column\n",
    "import matplotlib.pyplot as plt\n",
    "for column in df.columns:\n",
    "    plt.figure(figsize=(10,5))\n",
    "    df[column].value_counts().plot(kind='bar')\n",
    "    plt.title(f'Distribution of {column}')\n",
    "    plt.xlabel(column)\n",
    "    plt.ylabel('Count')\n",
    "    plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "c88ae4a9",
   "metadata": {
    "id": "c88ae4a9"
   },
   "source": [
    "# Algorithm 2"
   ]
  },
  {
   "cell_type": "code",
   "id": "05e73290",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "05e73290",
    "outputId": "9b67e559-52be-4e2c-b71d-66436fded4a2"
   },
   "source": [
    "#save unqiq values of each column to as a string array\n",
    "unique_values = {}\n",
    "for column in df.columns:\n",
    "    unique_values[column] = df[column].unique().tolist()\n",
    "    print(f\"Unique values in {column}: {unique_values[column]}\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "68ddcc79",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "68ddcc79",
    "outputId": "a41cec37-3fd2-48a9-a978-e6674b7be506"
   },
   "source": [
    "import json\n",
    "\n",
    "with open('taxonomy-tree.json', 'r') as f:\n",
    "    taxonomy_dict = json.load(f)\n",
    "\n",
    "# Display the dictionary\n",
    "taxonomy_dict"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "99677652",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "99677652",
    "outputId": "e5aa1765-3390-416b-bfc7-8d981c30ceab"
   },
   "source": [
    "#invert the taxonomy dictionary to get child to parent mapping\n",
    "def invert_taxonomy_tree(taxonomy, path=[]):\n",
    "    inverted_taxonomy_tree = {}\n",
    "    for key in taxonomy:\n",
    "\n",
    "\n",
    "        extended_path = [key] + path\n",
    "        # if(key == \"workclass\"):\n",
    "        #     print(taxonomy[key])\n",
    "        #     print(path)\n",
    "        # print(f\"Current key: {key}, Current path: {path}\")\n",
    "        if isinstance(taxonomy[key], dict):\n",
    "            result = invert_taxonomy_tree(taxonomy[key], extended_path)\n",
    "            inverted_taxonomy_tree.update(result)\n",
    "            # if(key == \"workclass\"):\n",
    "            #     print(inverted_taxonomy_tree)\n",
    "\n",
    "        if isinstance(taxonomy[key], list):\n",
    "            for item in taxonomy[key]:\n",
    "                inverted_taxonomy_tree[item] = [item] + extended_path\n",
    "\n",
    "        # path = []\n",
    "    return inverted_taxonomy_tree\n",
    "\n",
    "inverted_taxonomy_tree = invert_taxonomy_tree(taxonomy_dict, [])\n",
    "for key in inverted_taxonomy_tree:\n",
    "    print(f\"{key}: {inverted_taxonomy_tree[key]}\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "b7cbdfd0",
   "metadata": {
    "id": "b7cbdfd0"
   },
   "source": [
    "## Provenence Set"
   ]
  },
  {
   "cell_type": "code",
   "id": "559ba2f9",
   "metadata": {
    "id": "559ba2f9"
   },
   "source": [
    "def provenance(values):\n",
    "    result = []\n",
    "    for value in values:\n",
    "        mapping = inverted_taxonomy_tree.get(value, None)\n",
    "        if mapping:\n",
    "            result = result + mapping[:-1]  # Exclude the original value\n",
    "    return result\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "b27497e6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b27497e6",
    "outputId": "9981d9db-6178-461a-e392-1e901673927a"
   },
   "source": [
    "df.head(1).values[0]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "dfea502b",
   "metadata": {
    "id": "dfea502b"
   },
   "source": [
    "\n",
    "first_row = list(df.head(1).values[0])\n",
    "first_row = [s.strip() for s in first_row if isinstance(s, str)]\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "3626cd9f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3626cd9f",
    "outputId": "dfff86a2-18e0-42a1-b0f8-21664c8f6705"
   },
   "source": [
    "provenance(first_row)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "5fb13663",
   "metadata": {
    "id": "5fb13663"
   },
   "source": [
    "education_tree = {\n",
    "        \"Secondary\": {\n",
    "            \"Junior\": [\"9th\", \"10th\"],\n",
    "            \"Senior\": [\"11th\", \"12th\"]\n",
    "        },\n",
    "        \"University\": {\n",
    "            \"Bachelor\": [\"Bachelor\"],\n",
    "            \"Graduate\": [\"Master\", \"Doctorate\"]\n",
    "        }\n",
    "}\n",
    "\n",
    "# education_tree = {\n",
    "#     \"Any_Education\": [\"hello\"]\n",
    "# }\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "265fcc91",
   "metadata": {
    "id": "265fcc91"
   },
   "source": [
    "## Algorith 2 characteristic_vector_converting"
   ]
  },
  {
   "cell_type": "code",
   "id": "adbcb4a4",
   "metadata": {
    "id": "adbcb4a4"
   },
   "source": [
    "def characteristic_vector_converting(taxonomy_tree, values, inverted_taxonomy_tree):\n",
    "    characteristic_vector = []\n",
    "    # print(f\"Values to process for characteristic vector: {values}\")\n",
    "    for val in values:\n",
    "        # print(f\"Processing value: {val}\")\n",
    "        val = val.strip()\n",
    "        last_value = inverted_taxonomy_tree[val][-1]\n",
    "        # print(f\"Processing value: {val}, Last value in taxonomy path: {last_value}\")\n",
    "        # Get the values in the inverted taxonomy tree that have key as the last value\n",
    "        arr = []\n",
    "        for k in inverted_taxonomy_tree:\n",
    "            mapping = inverted_taxonomy_tree[k]\n",
    "            if mapping and mapping[-1] == last_value:\n",
    "                arr.append(mapping[:-1])  # Exclude the original value\n",
    "\n",
    "        arr = np.array(arr)\n",
    "        # print(arr.shape)\n",
    "        # print(arr)\n",
    "        # for each column in arr\n",
    "        # print(arr.shape[1])\n",
    "        provenance_set = provenance([val])\n",
    "        for col in range(len(provenance_set)):  # every column except last column\n",
    "            # Get the unique values in the column\n",
    "            coloumn_values = arr[:, col]\n",
    "            unique_values = np.unique(coloumn_values)\n",
    "            # print(f\"Unique values in column {provenance_set[col]}: {unique_values}\")\n",
    "            # print(f\"Value to encode: {unique_values[0]}\")\n",
    "            vector = [0] * len(unique_values)\n",
    "            # print(val)\n",
    "            index = np.where(unique_values == provenance_set[col])[0][0]\n",
    "            # print(f\"Index of value {val} in unique values: {index}\")\n",
    "            vector[index] = 1\n",
    "            characteristic_vector.extend(vector)\n",
    "            # Do something with the vector\n",
    "    return characteristic_vector\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "6ed52076",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6ed52076",
    "outputId": "df325f6a-d538-4a08-f390-1ab28bdb12a8"
   },
   "source": [
    "first_row[-2:]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "4ad644f0",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4ad644f0",
    "outputId": "0bd99234-a562-4a75-c8b7-31d32464cba8"
   },
   "source": [
    "characteristic_vector_converting(taxonomy_dict, first_row, inverted_taxonomy_tree)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "8m25lFVOVtak",
   "metadata": {
    "id": "8m25lFVOVtak"
   },
   "source": [
    "# Experiment parameters"
   ]
  },
  {
   "cell_type": "code",
   "id": "19d50866",
   "metadata": {
    "id": "19d50866"
   },
   "source": [
    "import random"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "Ab7D2MrUVwkI",
   "metadata": {
    "id": "Ab7D2MrUVwkI"
   },
   "source": [
    "# k=??\n",
    "# theta=??\n",
    "alpha=50"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "hQC_6hJyBj4U",
   "metadata": {
    "id": "hQC_6hJyBj4U"
   },
   "source": [
    "# Creating set of hash functions (universal use)"
   ]
  },
  {
   "cell_type": "code",
   "id": "_OIyoKu0BdKF",
   "metadata": {
    "id": "_OIyoKu0BdKF"
   },
   "source": [
    "\n",
    "# creates set F of hash functions Hash functions in F are in\n",
    "#  the form of h(x)=(ax+b) mod NPrime, where a and b are\n",
    "#  random integers, and NPrime is the smallest prime number\n",
    "#  larger than |U|.\n",
    "\n",
    "# F is a list of hash functions\n",
    "# Each hash function h takes an input x (row index in the characteristic vector)\n",
    "# and outputs a hashed value modulo a large prime number.\n",
    "# Example: h(x) = (a * x + b) % N_prime\n",
    "U=len(characteristic_vector_converting(taxonomy_dict, first_row, inverted_taxonomy_tree))\n",
    "random.seed(42)\n",
    "def create_hash_fam(num_hashes,U_size):\n",
    "  def is_prime(n):\n",
    "    if n<2:\n",
    "      return False\n",
    "\n",
    "    for i in range(2,int(n**0.5)+1):\n",
    "      if n%i==0:\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "  def next_prime(n):\n",
    "    while not is_prime(n):\n",
    "      n=n+1\n",
    "    return n\n",
    "\n",
    "  N_prime=next_prime(U_size +1)\n",
    "\n",
    "  F=[]\n",
    "\n",
    "  for i in range(num_hashes):\n",
    "    a=random.randint(1,N_prime-1)\n",
    "    b=random.randint(0,N_prime-1)\n",
    "    h = lambda x, a=a, b=b, N_prime=N_prime: (a * x + b) % N_prime\n",
    "    F.append(h)\n",
    "  return F\n",
    "F = create_hash_fam(num_hashes=alpha, U_size=U)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "XOmPRuyq3mcW",
   "metadata": {
    "id": "XOmPRuyq3mcW"
   },
   "source": [
    "# Algorithm 4"
   ]
  },
  {
   "cell_type": "code",
   "id": "lpOm88awI6iX",
   "metadata": {
    "id": "lpOm88awI6iX"
   },
   "source": [
    "import random"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "SejYHmSw886B",
   "metadata": {
    "id": "SejYHmSw886B"
   },
   "source": [
    "from math import inf\n",
    "###ALGORITHM 4\n",
    "def minhash(characteristic_vec,h_ab):\n",
    "  min_hash=float('inf')\n",
    "  for i,bit in enumerate(characteristic_vec):\n",
    "      if bit==1:\n",
    "        index=h_ab(i+1)\n",
    "        if index<min_hash:\n",
    "          min_hash=index\n",
    "\n",
    "  return min_hash\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "zHzUpaaMFY-3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zHzUpaaMFY-3",
    "outputId": "e7e1d410-de15-4303-bed2-c471077d7730"
   },
   "source": [
    "# testing algo 4\n",
    "# h_ab=F[random.randint(0,len(F)-1)]\n",
    "characteristic_vec_eg=characteristic_vector_converting(taxonomy_dict, first_row, inverted_taxonomy_tree)\n",
    "second_row = list(df.values[100])\n",
    "second_row = [s.strip() for s in second_row if isinstance(s, str)]\n",
    "vec1=characteristic_vector_converting(taxonomy_dict, first_row, inverted_taxonomy_tree)\n",
    "vec2=characteristic_vector_converting(taxonomy_dict, second_row, inverted_taxonomy_tree)\n",
    "min_hash1_val = minhash(vec1,F[2])\n",
    "min_hash2_val= minhash(vec2,F[2])\n",
    "\n",
    "print(min_hash1_val,min_hash2_val)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "w1iTX3O13QqV",
   "metadata": {
    "id": "w1iTX3O13QqV"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "DBnHSaK3wIy4",
   "metadata": {
    "id": "DBnHSaK3wIy4"
   },
   "source": [
    "## TAXONOMY Helper funcs"
   ]
  },
  {
   "cell_type": "code",
   "id": "5mXL2Tb8UG41",
   "metadata": {
    "id": "5mXL2Tb8UG41"
   },
   "source": [
    "# find the least common ancestor\n",
    "def find_lca(v1, v2, inverted_taxonomy_tree):\n",
    "  if v1 == v2:\n",
    "    return v1\n",
    "\n",
    "# get paths for both values\n",
    "  path1 = inverted_taxonomy_tree.get(v1, [])\n",
    "  path2 = inverted_taxonomy_tree.get(v2, [])\n",
    "\n",
    "  if not path1 or not path2:\n",
    "    return None #different attributes have no common ancestor\n",
    "\n",
    "  if path1[-1] != path2[-1]:\n",
    "    return None\n",
    "\n",
    "  # compare paths to find the lca\"\n",
    "  lca = None\n",
    "  min_len = min(len(path1), len(path2))\n",
    "\n",
    "  for i in range(1, min_len):\n",
    "    idx = -i - 1\n",
    "    if path1[idx] == path2[idx]:\n",
    "      lca = path1[idx]\n",
    "    else:\n",
    "      break\n",
    "\n",
    "  return lca\n",
    "\n",
    "def get_tree_height(attr_name, inverted_taxonomy_tree):\n",
    "\n",
    "  max_height = 0\n",
    "\n",
    "  for value, path in inverted_taxonomy_tree.items():\n",
    "        # Check if this value belongs to the attribute\n",
    "      if path and path[-1] == attr_name:\n",
    "            # Height = length of path minus 1 (exclude attribute name root)\n",
    "          height = len(path) - 1\n",
    "          max_height = max(max_height, height)\n",
    "\n",
    "  return max_height\n",
    "\n",
    "def path_length_between(v1, v2, inverted_taxonomy_tree):\n",
    "\n",
    "    if v1 == v2:\n",
    "        return 0\n",
    "\n",
    "    lca = find_lca(v1, v2, inverted_taxonomy_tree)\n",
    "\n",
    "    if lca is None:\n",
    "        # No common ancestor meaning different attributes or invalid\n",
    "        return float('inf')\n",
    "\n",
    "    path1 = inverted_taxonomy_tree.get(v1, [])\n",
    "    path2 = inverted_taxonomy_tree.get(v2, [])\n",
    "\n",
    "    # Find distance from v1 to LCA\n",
    "    # Count steps from v1 (index 0) to LCA\n",
    "    try:\n",
    "        lca_index_in_path1 = path1.index(lca)\n",
    "        dist1 = lca_index_in_path1\n",
    "    except ValueError:\n",
    "        dist1 = 0\n",
    "\n",
    "    # distance from v2 to LCA\n",
    "    try:\n",
    "        lca_index_in_path2 = path2.index(lca)\n",
    "        dist2 = lca_index_in_path2\n",
    "    except ValueError:\n",
    "        dist2 = 0\n",
    "\n",
    "    # Total path length\n",
    "    L = dist1 + dist2\n",
    "\n",
    "    return L\n",
    "\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "hDFBP34w3aqo",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hDFBP34w3aqo",
    "outputId": "73e44eb0-4537-4ac5-d209-0607faf4d905"
   },
   "source": [
    "# confirmations\n",
    "\n",
    "v1 = \"State-gov\"\n",
    "v2 = \"Federal-gov\"\n",
    "lca = find_lca(v1, v2, inverted_taxonomy_tree)\n",
    "print(f\"LCA of '{v1}' and '{v2}': {lca}\")\n",
    "print(f\"Expected: 'Government'\")\n",
    "print(f\"Path1: {inverted_taxonomy_tree[v1]}\")\n",
    "print(f\"Path2: {inverted_taxonomy_tree[v2]}\")\n",
    "\n",
    "v1 = \"State-gov\"\n",
    "v2 = \"Private\"\n",
    "lca = find_lca(v1, v2, inverted_taxonomy_tree)\n",
    "print(f\"\\nLCA of '{v1}' and '{v2}': {lca}\")\n",
    "print(f\"Expected: 'workclass' \")\n",
    "print(f\"Path1: {inverted_taxonomy_tree[v1]}\")\n",
    "print(f\"Path2: {inverted_taxonomy_tree[v2]}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "CnaxLlHF3fYy",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CnaxLlHF3fYy",
    "outputId": "f8ef33ca-fcd6-4003-ea93-cedc96ee147a"
   },
   "source": [
    "v1 = \"Masters\"\n",
    "v2 = \"Doctorate\"\n",
    "lca = find_lca(v1, v2, inverted_taxonomy_tree)\n",
    "print(f\"LCA of '{v1}' and '{v2}': {lca}\")\n",
    "print(f\"Expected: 'Graduate'\")\n",
    "print(f\"Path1: {inverted_taxonomy_tree[v1]}\")\n",
    "print(f\"Path2: {inverted_taxonomy_tree[v2]}\")\n",
    "\n",
    "v1 = \"Masters\"\n",
    "v2 = \"Bachelors\"\n",
    "lca = find_lca(v1, v2, inverted_taxonomy_tree)\n",
    "print(f\"\\nLCA of '{v1}' and '{v2}': {lca}\")\n",
    "print(f\"Expected: 'University'\")\n",
    "print(f\"Path1: {inverted_taxonomy_tree[v1]}\")\n",
    "print(f\"Path2: {inverted_taxonomy_tree[v2]}\")\n",
    "\n",
    "v1 = \"Masters\"\n",
    "v2 = \"HS-grad\"\n",
    "lca = find_lca(v1, v2, inverted_taxonomy_tree)\n",
    "print(f\"\\nLCA of '{v1}' and '{v2}': {lca}\")\n",
    "print(f\"Expected: 'education' or 'Post-Secondary' and 'Secondary' common parent\")\n",
    "print(f\"Path1: {inverted_taxonomy_tree[v1]}\")\n",
    "print(f\"Path2: {inverted_taxonomy_tree[v2]}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "ghOMEpRS3hta",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ghOMEpRS3hta",
    "outputId": "4d486e4b-8f21-4140-8834-ee68925eca36"
   },
   "source": [
    "# tree height\n",
    "\n",
    "for attr in ['workclass', 'education', 'sex', 'race', 'marital-status', 'occupation']:\n",
    "    height = get_tree_height(attr, inverted_taxonomy_tree)\n",
    "    print(f\"Height of '{attr}' taxonomy tree: {height}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "w7U75RSE3jso",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "w7U75RSE3jso",
    "outputId": "16e8ec3f-bb53-4ba4-fec0-1abe6f28e9aa"
   },
   "source": [
    "# Same values\n",
    "v1 = \"Masters\"\n",
    "v2 = \"Masters\"\n",
    "L = path_length_between(v1, v2, inverted_taxonomy_tree)\n",
    "print(f\"Path length between '{v1}' and '{v2}': {L}\")\n",
    "print(f\"Expected: 0 (same value)\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "YgiGS-Ju3mLX",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YgiGS-Ju3mLX",
    "outputId": "e40c3d59-40e2-4152-aa7c-b2efb9224ae6"
   },
   "source": [
    "# too far apart\n",
    "v1 = \"Masters\"\n",
    "v2 = \"HS-grad\"\n",
    "L = path_length_between(v1, v2, inverted_taxonomy_tree)\n",
    "print(f\"\\nPath length between '{v1}' and '{v2}': {L}\")\n",
    "print(f\"Path1: {inverted_taxonomy_tree[v1]}\")\n",
    "print(f\"Path2: {inverted_taxonomy_tree[v2]}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "OXlFxjzM3r1E",
   "metadata": {
    "id": "OXlFxjzM3r1E"
   },
   "source": [
    "# Distance"
   ]
  },
  {
   "cell_type": "code",
   "id": "KLYRrHO-3m_3",
   "metadata": {
    "id": "KLYRrHO-3m_3"
   },
   "source": [
    "def categorical_distance(v1, v2, attr_name, inverted_taxonomy_tree):\n",
    "    \"\"\"\n",
    "    Equation (5): Path-based distance between two categorical values\n",
    "    d(v, v') = L(v, v') / (2H)\n",
    "\n",
    "    \"\"\"\n",
    "    if v1 == v2:\n",
    "        return 0.0\n",
    "\n",
    "    # Get path length L(v1, v2)\n",
    "    L = path_length_between(v1, v2, inverted_taxonomy_tree)\n",
    "\n",
    "    if L == float('inf'):\n",
    "        return 1.0\n",
    "\n",
    "    # Get tree height H\n",
    "    H = get_tree_height(attr_name, inverted_taxonomy_tree)\n",
    "\n",
    "    if H == 0:\n",
    "        return 0.0\n",
    "\n",
    "    # Return normalized distance\n",
    "    distance = L / (2.0 * H)\n",
    "\n",
    "    return distance\n",
    "\n",
    "\n",
    "def qid_distance(qid1, qid2, attr_names, inverted_taxonomy_tree, weights=None):\n",
    "    \"\"\"\n",
    "    Equation (6): Distance between two quasi-identifiers (records)\n",
    "    d(qid, qid') = Σ(ωᵢ × d(vᵢ, v'ᵢ))\n",
    "\n",
    "    \"\"\"\n",
    "    m = len(qid1)\n",
    "\n",
    "    if weights is None:\n",
    "        weights = [1.0 / m] * m\n",
    "\n",
    "    total_distance = 0.0\n",
    "\n",
    "    for i in range(m):\n",
    "        # Calculate distance for this attribute\n",
    "        cat_dist = categorical_distance(\n",
    "            qid1[i],\n",
    "            qid2[i],\n",
    "            attr_names[i],\n",
    "            inverted_taxonomy_tree\n",
    "        )\n",
    "        total_distance += weights[i] * cat_dist\n",
    "\n",
    "    return total_distance\n",
    "\n",
    "\n",
    "def cluster_distance(cluster1, cluster2, k, attr_names, inverted_taxonomy_tree, theta=None):\n",
    "    \"\"\"\n",
    "    Equation (8): Flexible distance between two clusters\n",
    "    d(C, C') = (θ × Δ + 1) × max{d(qid, qid')}\n",
    "    where Δ = |C| + |C'| - k\n",
    "\n",
    "    \"\"\"\n",
    "    if theta is None:\n",
    "        theta = 1.0 / k\n",
    "\n",
    "    # Calculate Δ (delta)\n",
    "    delta = len(cluster1) + len(cluster2) - k\n",
    "\n",
    "    # Find maximum pairwise distance (diameter of merged cluster)\n",
    "    max_distance = 0.0\n",
    "    for qid1 in cluster1.records:\n",
    "        for qid2 in cluster2.records:\n",
    "            dist = qid_distance(qid1, qid2, attr_names, inverted_taxonomy_tree)\n",
    "            max_distance = max(max_distance, dist)\n",
    "\n",
    "    # Apply flexible distance formula\n",
    "    flexible_distance = (theta * delta + 1) * max_distance\n",
    "\n",
    "    return flexible_distance"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "EL7XOlHR30aG",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EL7XOlHR30aG",
    "outputId": "f1ff191c-40b4-42a0-9ea7-6175b35e188b"
   },
   "source": [
    "print(\"TEST 2: DISTANCE FUNCTIONS\")\n",
    "\n",
    "categorical_cols = ['workclass', 'education', 'marital-status', 'occupation',\n",
    "                    'relationship', 'race', 'sex', 'native-country']\n",
    "\n",
    "# Get first two records\n",
    "record1 = df[categorical_cols].iloc[0].tolist()\n",
    "record2 = df[categorical_cols].iloc[1].tolist()\n",
    "\n",
    "# Strip whitespace\n",
    "record1 = [s.strip() if isinstance(s, str) else s for s in record1]\n",
    "record2 = [s.strip() if isinstance(s, str) else s for s in record2]\n",
    "\n",
    "print(\"\\n[Test 2.1] Categorical Distance\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"Record 1 workclass: {record1[0]}\")\n",
    "print(f\"Record 2 workclass: {record2[0]}\")\n",
    "\n",
    "dist = categorical_distance(record1[0], record2[0], 'workclass', inverted_taxonomy_tree)\n",
    "print(f\"Categorical distance: {dist:.4f}\")\n",
    "\n",
    "print(f\"\\nRecord 1 education: {record1[1]}\")\n",
    "print(f\"Record 2 education: {record2[1]}\")\n",
    "dist = categorical_distance(record1[1], record2[1], 'education', inverted_taxonomy_tree)\n",
    "print(f\"Categorical distance: {dist:.4f}\")\n",
    "\n",
    "print(\"\\n[Test 2.2] QID Distance\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"Record 1: {record1}\")\n",
    "print(f\"Record 2: {record2}\")\n",
    "dist = qid_distance(record1, record2, categorical_cols, inverted_taxonomy_tree)\n",
    "print(f\"QID distance: {dist:.4f}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "sIJ5twFZ33n6",
   "metadata": {
    "id": "sIJ5twFZ33n6"
   },
   "source": [
    "class Cluster:\n",
    "    \"\"\"\n",
    "    Represents a cluster of quasi-identifiers (data records)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, records):\n",
    "\n",
    "        self.records = records\n",
    "\n",
    "    def __len__(self):\n",
    "        # Return number of records in cluster\n",
    "        return len(self.records)\n",
    "\n",
    "    def merge(self, other_cluster):\n",
    "\n",
    "        # Merge two clusters into one\n",
    "\n",
    "        return Cluster(self.records + other_cluster.records)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"Cluster(size={len(self.records)})\""
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "7rdHTnWc35w4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7rdHTnWc35w4",
    "outputId": "9af2d2b1-50a9-4aa6-e88c-d1d883f9fc5b"
   },
   "source": [
    "print(\"TEST 3: CLUSTER CLASS\")\n",
    "\n",
    "\n",
    "# Create test clusters\n",
    "records1 = [record1, record2]\n",
    "records2 = df[categorical_cols].iloc[2:4].values.tolist()\n",
    "records2 = [[s.strip() if isinstance(s, str) else s for s in rec] for rec in records2]\n",
    "\n",
    "cluster1 = Cluster(records1)\n",
    "cluster2 = Cluster(records2)\n",
    "\n",
    "print(f\"\\nCluster 1: {cluster1}\")\n",
    "print(f\"  Records: {len(cluster1.records)}\")\n",
    "\n",
    "print(f\"\\nCluster 2: {cluster2}\")\n",
    "print(f\"  Records: {len(cluster2.records)}\")\n",
    "\n",
    "# Test merge\n",
    "merged = cluster1.merge(cluster2)\n",
    "print(f\"\\nMerged cluster: {merged}\")\n",
    "print(f\"  Records: {len(merged.records)}\")\n",
    "print(f\"Expected: 4 records\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "153327ab",
   "metadata": {
    "id": "153327ab"
   },
   "source": [
    "def _cluster_distance(self, cluster1, cluster2, k, theta):\n",
    "        \"\"\"\n",
    "        Internal method to compute cluster distance using stored taxonomy tree\n",
    "        \"\"\"\n",
    "        return cluster_distance(\n",
    "            cluster1,\n",
    "            cluster2,\n",
    "            k,\n",
    "            self.attribute_names,\n",
    "            self.inverted_taxonomy_tree,\n",
    "            theta\n",
    "        )\n",
    "\n",
    "def _qid_distance(self, qid1, qid2, weights=None):\n",
    "        \"\"\"\n",
    "        Internal method to compute QID distance using stored taxonomy tree\n",
    "        \"\"\"\n",
    "        return qid_distance(\n",
    "            qid1,\n",
    "            qid2,\n",
    "            self.attribute_names,\n",
    "            self.inverted_taxonomy_tree,\n",
    "            weights\n",
    "        )\n",
    "\n",
    "def _categorical_distance(self, v1, v2, attribute_name):\n",
    "        \"\"\"\n",
    "        Internal method to compute categorical distance using stored taxonomy tree\n",
    "        \"\"\"\n",
    "        return categorical_distance(\n",
    "            v1,\n",
    "            v2,\n",
    "            attribute_name,\n",
    "            self.inverted_taxonomy_tree\n",
    "        )"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "bfdd2f73",
   "metadata": {
    "id": "bfdd2f73"
   },
   "source": [
    "import heapq\n",
    "\n",
    "class BetaACClustering:\n",
    "\n",
    "    def __init__(self, inverted_taxonomy_tree, attribute_names):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            inverted_taxonomy_tree: dict mapping values to taxonomy paths\n",
    "                Example: {'Masters': ['Masters', 'Graduate', 'University', 'Post-Secondary', 'education']}\n",
    "            attribute_names: list of attribute names in order\n",
    "                Example: ['sex', 'zipcode', 'education', 'marital-status', ...]\n",
    "        \"\"\"\n",
    "        self.inverted_taxonomy_tree = inverted_taxonomy_tree\n",
    "        self.attribute_names = attribute_names\n",
    "\n",
    "    def _cluster_distance(self, cluster1, cluster2, k, theta):\n",
    "        \"\"\"\n",
    "        Internal method to compute cluster distance using stored taxonomy tree\n",
    "        \"\"\"\n",
    "        return cluster_distance(\n",
    "            cluster1,\n",
    "            cluster2,\n",
    "            k,\n",
    "            self.attribute_names,\n",
    "            self.inverted_taxonomy_tree,\n",
    "            theta\n",
    "        )\n",
    "\n",
    "    # Strictly according to algorithm 6, line per line\n",
    "    def beta_ac(self, small_clusters, k, theta=None):\n",
    "        \"\"\"\n",
    "        Algorithm 6: Beta-AC (β-cluster Agglomerative Clustering)\n",
    "\n",
    "        INPUT:\n",
    "            small_clusters: list of Cluster objects (each size < k)\n",
    "            k: privacy parameter (minimum cluster size)\n",
    "            theta: weight parameter (default: 1/k)\n",
    "\n",
    "        OUTPUT:\n",
    "            tuple: (k_member_clusters, remaining_cluster)\n",
    "                - k_member_clusters: list of Cluster objects (size >= k)\n",
    "                - remaining_cluster: single Cluster object (size < k) or None\n",
    "        \"\"\"\n",
    "        if theta is None:\n",
    "            theta = 1.0 / k\n",
    "\n",
    "        # Initialize outputs\n",
    "        k_member_clusters = []\n",
    "        remaining_cluster = None\n",
    "\n",
    "        # Active clusters (use dict for easy deletion)\n",
    "        active_clusters = {i: cluster for i, cluster in enumerate(small_clusters)}\n",
    "\n",
    "        # Priority queue: (distance, cluster_id_1, cluster_id_2)\n",
    "        pqueue = []\n",
    "\n",
    "        # Line 1: Compute all pairwise distances and populate priority queue\n",
    "        cluster_ids = list(active_clusters.keys())\n",
    "        for i in range(len(cluster_ids)):\n",
    "            for j in range(i + 1, len(cluster_ids)):\n",
    "                id1, id2 = cluster_ids[i], cluster_ids[j]\n",
    "                dist = self._cluster_distance(\n",
    "                    active_clusters[id1],\n",
    "                    active_clusters[id2],\n",
    "                    k,\n",
    "                    theta\n",
    "                )\n",
    "                heapq.heappush(pqueue, (dist, id1, id2))\n",
    "\n",
    "        # Track next available cluster ID\n",
    "        next_id = max(active_clusters.keys()) + 1 if active_clusters else 0\n",
    "\n",
    "        # Line 2: Main merging loop\n",
    "        while pqueue:\n",
    "            # Line 3: Extract pair with minimum distance\n",
    "            dist, id_x, id_y = heapq.heappop(pqueue)\n",
    "\n",
    "            # Skip if either cluster was already merged\n",
    "            if id_x not in active_clusters or id_y not in active_clusters:\n",
    "                continue\n",
    "\n",
    "            # Get clusters and merge\n",
    "            cluster_x = active_clusters[id_x]\n",
    "            cluster_y = active_clusters[id_y]\n",
    "            cluster_z = cluster_x.merge(cluster_y)\n",
    "\n",
    "            # Line 4: Remove old clusters from active set\n",
    "            del active_clusters[id_x]\n",
    "            del active_clusters[id_y]\n",
    "\n",
    "            # Line 5: Delete entries from priority queue happens implicitly\n",
    "            # (we skip invalid pairs in the loop above)\n",
    "\n",
    "            # Line 6-10: Handle merged cluster based on size\n",
    "            if len(cluster_z) >= k:\n",
    "                # Line 7: Add to k-member clusters (done!)\n",
    "                k_member_clusters.append(cluster_z)\n",
    "            else:\n",
    "                # Line 9: Add back to active clusters (needs more merging)\n",
    "                new_id = next_id\n",
    "                next_id += 1\n",
    "                active_clusters[new_id] = cluster_z\n",
    "\n",
    "                # Line 10: Update priority queue with new distances\n",
    "                for other_id, other_cluster in active_clusters.items():\n",
    "                    if other_id != new_id:\n",
    "                        new_dist = self._cluster_distance(\n",
    "                            cluster_z,\n",
    "                            other_cluster,\n",
    "                            k,\n",
    "                            theta\n",
    "                        )\n",
    "                        heapq.heappush(pqueue, (new_dist, new_id, other_id))\n",
    "\n",
    "        # Line 11: Handle remaining cluster (if exactly one left)\n",
    "        if len(active_clusters) == 1:\n",
    "            remaining_id = list(active_clusters.keys())[0]\n",
    "            remaining_cluster = active_clusters[remaining_id]\n",
    "\n",
    "        return k_member_clusters, remaining_cluster\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "7582bb26",
   "metadata": {
    "id": "7582bb26"
   },
   "source": [
    "# Define attribute names (columns you're using for quasi-identifiers)\n",
    "attribute_names = ['workclass', 'education', 'marital-status', 'occupation',\n",
    "                   'relationship', 'race', 'sex', 'native-country']\n",
    "\n",
    "# Initialize once\n",
    "clusterer = BetaACClustering(inverted_taxonomy_tree, attribute_names)\n",
    "\n",
    "# Prepare your data as Cluster objects\n",
    "# Example: Convert dataframe rows to clusters\n",
    "print(len(df))\n",
    "small_clusters = []\n",
    "for i in range(0, 1500, 2):  # Group every 2 records as a small cluster\n",
    "    records = df.iloc[i:i+2][attribute_names].values.tolist()\n",
    "    # Strip whitespace from strings\n",
    "    records = [[s.strip() if isinstance(s, str) else s for s in record] for record in records]\n",
    "    small_clusters.append(Cluster(records))\n",
    "\n",
    "# Run Algorithm 6 (matches paper specification exactly!)\n",
    "k = 10\n",
    "k_member_clusters, remaining_cluster = clusterer.beta_ac(small_clusters, k)\n",
    "\n",
    "# Check results\n",
    "print(f\"Created {len(k_member_clusters)} k-member clusters\")\n",
    "for i, cluster in enumerate(k_member_clusters):\n",
    "    print(f\"Cluster {i+1}: {len(cluster)} records\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "CzcjLJYs3_eM",
   "metadata": {
    "id": "CzcjLJYs3_eM"
   },
   "source": [
    "print(\"TEST 4: ALGORITHM 6 - BETA-AC\")\n",
    "\n",
    "# Prepare small clusters\n",
    "k = 5\n",
    "n_records = 20\n",
    "\n",
    "sample_data = df[categorical_cols].iloc[:n_records].values.tolist()\n",
    "sample_data = [[s.strip() if isinstance(s, str) else s for s in rec] for rec in sample_data]\n",
    "\n",
    "# small clusters (each < k)\n",
    "small_clusters = []\n",
    "cluster_size = 2  # Each initial cluster has 2 records\n",
    "\n",
    "for i in range(0, n_records, cluster_size):\n",
    "    records = sample_data[i:i+cluster_size]\n",
    "    if records:\n",
    "        small_clusters.append(Cluster(records))\n",
    "\n",
    "print(f\"\\n[Input]\")\n",
    "print(f\"k = {k}\")\n",
    "print(f\"Number of small clusters: {len(small_clusters)}\")\n",
    "print(f\"Small cluster sizes: {[len(c) for c in small_clusters]}\")\n",
    "\n",
    "k_member_clusters, remaining_cluster = clusterer.beta_ac(\n",
    "    small_clusters,\n",
    "    k\n",
    ")\n",
    "\n",
    "print(f\"\\n[Output]\")\n",
    "print(f\"Number of k-member clusters: {len(k_member_clusters)}\")\n",
    "print(f\"k-member cluster sizes: {[len(c) for c in k_member_clusters]}\")\n",
    "\n",
    "if remaining_cluster:\n",
    "    print(f\"Remaining cluster size: {len(remaining_cluster)}\")\n",
    "else:\n",
    "    print(f\"No remaining cluster\")\n",
    "\n",
    "# Verify constraints\n",
    "print(f\"\\n[Verification]\")\n",
    "all_valid = True\n",
    "for i, cluster in enumerate(k_member_clusters):\n",
    "    size = len(cluster)\n",
    "    is_valid = k <= size <= 2*k - 1\n",
    "    print(f\"Cluster {i}: size={size}, valid={is_valid} (should be {k} <= size <= {2*k-1})\")\n",
    "    if not is_valid:\n",
    "        all_valid = False\n",
    "\n",
    "if remaining_cluster and len(remaining_cluster) >= k:\n",
    "    print(f\"WARNING: Remaining cluster has size >= k!\")\n",
    "    all_valid = False\n",
    "\n",
    "print(f\"\\nAll constraints satisfied: {all_valid}\")\n",
    "\n",
    "# Check total records preserved\n",
    "total_input = sum(len(c) for c in small_clusters)\n",
    "total_output = sum(len(c) for c in k_member_clusters)\n",
    "if remaining_cluster:\n",
    "    total_output += len(remaining_cluster)\n",
    "\n",
    "print(f\"\\nRecords in: {total_input}\")\n",
    "print(f\"Records out: {total_output}\")\n",
    "print(f\"All records preserved: {total_input == total_output}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "c5ab6fa9",
   "metadata": {},
   "source": [
    "\n",
    "# === Implementations of Algorithm 3 (Map) and Algorithm 5 (LSH-RC) using notebook functions ===\n",
    "# This cell integrates with existing notebook variables/functions:\n",
    "# - characteristic_vector_converting(taxonomy_dict, row_values, inverted_taxonomy_tree)\n",
    "# - minhash(characteristic_vec, h_ab)  # h_ab is a single hash function from F\n",
    "# - F : list of hash functions (length = alpha)\n",
    "# - taxonomy_dict, inverted_taxonomy_tree : available taxonomy structures\n",
    "# - alpha : banding parameter (number of hash rows used to form bucketID)\n",
    "# - k : privacy parameter\n",
    "#\n",
    "# The functions below follow the paper's pseudocode and use the notebook's helpers.\n",
    "\n",
    "from collections import defaultdict, deque\n",
    "import heapq\n",
    "\n",
    "def algorithm_3_map(record_id, row_values, taxonomy_dict, inverted_taxonomy_tree, F, alpha):\n",
    "    \"\"\"\n",
    "    Implements Algorithm 3 (Map): convert record -> characteristic vector -> compute alpha MinHash values\n",
    "    and concatenate them to create bucketID. Returns (bucketID, (record_id, row_values))\n",
    "    \"\"\"\n",
    "    # convert to characteristic vector using existing function\n",
    "    char_vec = characteristic_vector_converting(taxonomy_dict, row_values, inverted_taxonomy_tree)\n",
    "    print(char_vec)\n",
    "    # compute minhash for each hash function in F (use first alpha entries)\n",
    "    if alpha <= 0:\n",
    "        raise ValueError(\"alpha must be positive\")\n",
    "    if len(F) < alpha:\n",
    "        raise ValueError(\"F must contain at least alpha hash functions\")\n",
    "    mins = []\n",
    "    for i in range(alpha):\n",
    "        h_ab = F[i]\n",
    "        mh = minhash(char_vec, h_ab)\n",
    "        mins.append(str(int(mh)))\n",
    "    bucket_id = \"-\".join(mins)\n",
    "    return bucket_id, (record_id, row_values, mins)\n",
    "\n",
    "def lsh_partitioning_serial(records, taxonomy_dict, inverted_taxonomy_tree, F, alpha):\n",
    "    \"\"\"\n",
    "    Serial emulation of Map+Shuffle: records is list of (record_id, row_values)\n",
    "    Returns dict: bucket_id -> list of (record_id, row_values)\n",
    "    \"\"\"\n",
    "    buckets = defaultdict(list)\n",
    "    for rid, row in records:\n",
    "        bid, payload = algorithm_3_map(rid, row, taxonomy_dict, inverted_taxonomy_tree, F, alpha)\n",
    "        buckets[bid].append((rid, row))\n",
    "    return buckets\n",
    "\n",
    "# Helper: provenance set based jaccard distance pairwise using notebook provenance()\n",
    "def provenance_set_of_row(row_values):\n",
    "    # expects 'row_values' aligned with taxonomy order as used by characteristic_vector_converting\n",
    "    # use existing 'provenance' helper that returns list of ancestors for each value\n",
    "    ps = []\n",
    "    for v in row_values:\n",
    "        try:\n",
    "            p = provenance([v])  # provenance returns list; we exclude original value in earlier helper, but for similarity consider combining root nodes\n",
    "            # provenance() in this notebook returns mapping excluding original value; to be consistent with paper, include the value itself\n",
    "            # so include v plus p\n",
    "            set_elems = [v] + p\n",
    "        except Exception:\n",
    "            set_elems = [v]\n",
    "        ps.extend(set_elems)\n",
    "    return set(ps)\n",
    "\n",
    "def jaccard_distance_sets(A, B):\n",
    "    if not A and not B: return 0.0\n",
    "    inter = len(A & B)\n",
    "    uni = len(A | B)\n",
    "    return 1.0 - inter/uni\n",
    "\n",
    "def cluster_diameter(cluster):\n",
    "    \"\"\"\n",
    "    cluster: list of (id, row_values)\n",
    "    compute max pairwise provenance-set jaccard distance\n",
    "    \"\"\"\n",
    "    provs = [provenance_set_of_row(row) for (_id,row) in cluster]\n",
    "    maxd = 0.0\n",
    "    for i in range(len(provs)):\n",
    "        for j in range(i+1, len(provs)):\n",
    "            d = jaccard_distance_sets(provs[i], provs[j])\n",
    "            if d > maxd: maxd = d\n",
    "    return maxd\n",
    "\n",
    "# Beta-AC implementation following Algorithm 6\n",
    "def beta_ac(small_clusters, k, theta=None):\n",
    "    \"\"\"\n",
    "    small_clusters: list of clusters, each cluster is list of (id,row)\n",
    "    returns (list_of_k_member_clusters, remaining_cluster_or_None)\n",
    "    \"\"\"\n",
    "    if theta is None:\n",
    "        theta = 1.0 / k\n",
    "    # Initialize clusters\n",
    "    clusters = [list(c) for c in small_clusters]\n",
    "    n = len(clusters)\n",
    "    if n == 0:\n",
    "        return [], None\n",
    "    # compute diameters\n",
    "    diam = [cluster_diameter(c) for c in clusters]\n",
    "    # priority queue of (distance, idx_a, idx_b)\n",
    "    pq = []\n",
    "    for i in range(n):\n",
    "        for j in range(i+1, n):\n",
    "            delta = len(clusters[i]) + len(clusters[j]) - k\n",
    "            dist = (theta * max(delta,0) + 1) * max(diam[i], diam[j], 1e-12)\n",
    "            heapq.heappush(pq, (dist, i, j))\n",
    "    active = {i: clusters[i] for i in range(n)}\n",
    "    removed = set()\n",
    "    results = []\n",
    "    next_idx = n\n",
    "    # Maintain diam dict for new clusters as they are created\n",
    "    diam_dict = {i: diam[i] for i in range(n)}\n",
    "    while pq:\n",
    "        dist, i, j = heapq.heappop(pq)\n",
    "        if i in removed or j in removed:\n",
    "            continue\n",
    "        # merge i and j\n",
    "        a = active.pop(i)\n",
    "        b = active.pop(j)\n",
    "        removed.add(i); removed.add(j)\n",
    "        merged = a + b\n",
    "        new_diam = cluster_diameter(merged)\n",
    "        if len(merged) >= k:\n",
    "            results.append(merged)\n",
    "        else:\n",
    "            # add back as active cluster with new index\n",
    "            idx_new = next_idx; next_idx += 1\n",
    "            active[idx_new] = merged\n",
    "            diam_dict[idx_new] = new_diam\n",
    "            # push distances against other active clusters\n",
    "            for other_idx in list(active.keys()):\n",
    "                if other_idx == idx_new: continue\n",
    "                delta = len(active[other_idx]) + len(merged) - k\n",
    "                dist2 = (theta * max(delta,0) + 1) * max(diam_dict[other_idx], new_diam, 1e-12)\n",
    "                heapq.heappush(pq, (dist2, other_idx, idx_new))\n",
    "    # choose remaining cluster if any active cluster < k remains\n",
    "    remaining = None\n",
    "    for c in active.values():\n",
    "        if 0 < len(c) < k:\n",
    "            remaining = c\n",
    "            break\n",
    "    return results, remaining\n",
    "\n",
    "# Algorithm 5: LSH-RC recursive clustering\n",
    "def algorithm_5_lsh_rc(C_records, k, alpha, taxonomy_dict, inverted_taxonomy_tree, F):\n",
    "    \"\"\"\n",
    "    C_records: list of (id, row_values)\n",
    "    Returns: (k_member_clusters_list, remaining_cluster_or_None)\n",
    "    \"\"\"\n",
    "    C_out = []\n",
    "    Cr = None\n",
    "    CS = []  # small clusters to be processed by beta_ac\n",
    "    if len(C_records) < k:\n",
    "        return [], C_records\n",
    "    if len(C_records) == k:\n",
    "        return [C_records], None\n",
    "    # partition using serial LSH partitioning\n",
    "    buckets = lsh_partitioning_serial(C_records, taxonomy_dict, inverted_taxonomy_tree, F, alpha)\n",
    "    for bid, bucket in buckets.items():\n",
    "        if len(bucket) < k:\n",
    "            CS.append(bucket)\n",
    "        elif len(bucket) == k:\n",
    "            C_out.append(bucket)\n",
    "        else:\n",
    "            # recursive call\n",
    "            cprime, rem = algorithm_5_lsh_rc(bucket, k, alpha, taxonomy_dict, inverted_taxonomy_tree, F)\n",
    "            if cprime:\n",
    "                C_out.extend(cprime)\n",
    "            if rem:\n",
    "                CS.append(rem)\n",
    "    # After processing buckets, run Beta-AC on CS\n",
    "    if CS:\n",
    "        ac_results, rem_cluster = beta_ac(CS, k)\n",
    "        if ac_results:\n",
    "            C_out.extend(ac_results)\n",
    "        Cr = rem_cluster\n",
    "    return C_out, Cr\n",
    "\n",
    "# Expose names for easy usage\n",
    "Algorithm3_map = algorithm_3_map\n",
    "LSH_RC = algorithm_5_lsh_rc\n",
    "Beta_AC = beta_ac\n",
    "\n",
    "print(\"Algorithm 3 and Algorithm 5 implementations loaded: Algorithm3_map, LSH_RC, Beta_AC\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pprint as pp\n",
    "records = [(str(i), list(df.iloc[i][categorical_cols])) for i in range(len(df))]\n",
    "\n",
    "# Running algorithm 3\n",
    "buckets = lsh_partitioning_serial(records, taxonomy_dict, inverted_taxonomy_tree, F, alpha)\n",
    "len(buckets), [ (k,len(v)) for k,v in buckets.items()][:10]\n",
    "\n",
    "# Running LSH-RC\n",
    "kmembers, remaining = LSH_RC(records, k, alpha, taxonomy_dict, inverted_taxonomy_tree, F)\n",
    "len(kmembers), remaining\n"
   ],
   "id": "9fca00f28440f6fa",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "name": "python3",
   "language": "python"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
